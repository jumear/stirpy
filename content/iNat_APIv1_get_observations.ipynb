{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API v1 Get Observations Example\n- Link: https://jumear.github.io/stirpy/lab?path=iNat_APIv1_get_observations.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Get Data from the iNaturalist API",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This example gets either a single or mutliple pages of results from the [API](https://api.inaturalist.org/). The requests are made asynchronously (in parallel, with a small incremental delay between the initiation of each page request), allowing large recordsets to be fetched in the shortest amount of time while respectng iNaturalist's [suggested request limit](https://www.inaturalist.org/pages/developers) (about 1 per second).\n\nThe example also provides a model for parsing the results, including flattening some of the items returned in the results (for use cases where the data is expected to be tabular, such as when exporting to a CSV file). For example, a single observation can be associated with multiple identifications, and this example code can flatten those multiple identifications into a single string so that all the identifications can be written out on a single line with the same observation row. This example also provides a model for client-side filtering (as an alternative when server-side filtering is not possible). The example shows how to get additional information, such as annotation descriptions and standard place information for observations.\n\nThere are also examples of how to get counts of observations or a series of counts (ex. observation counts by state).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\nfrom functools import partial # used for pre-loading functions with some arguments\nfrom datetime import datetime # used to convert string datetimes into actual datetimes\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define custom functions used for getting data\n\ndef params_to_dict(params_string):\n    \"\"\"Convert a parameter string into a dict.\n    ex.: 'taxon_id=1&user_id=kueda,loarie' => {'taxon_id': ['1'], 'user_id': ['kueda', 'loarie']} \n    \"\"\"\n    params_dict = parse_qs(params_string)\n    for p, v in params_dict.items():\n        if v: # iNaturalist handles multiple values for the same parameter using comma separated values. since parse_qs doesn't handle that situation, this section will handle it.\n            v = [(vv.split(',') if vv else vv) for vv in v]\n            params_dict[p] = [vvv for vv in v for vvv in vv]\n    return params_dict\n\ndef url_with_params(url_base, params=None):\n    \"\"\"Combine a base url with a set of parameters. Can handle the following types of cases:\n    1. 'https://api.inaturalist.org/v1/observations' + {'taxon_id': [1], 'user_id': ['kueda','loarie']} => 'https://api.inaturlaist.org/v1/observations?taxon_id=1&user_id=kueda,loarie'\n    2. 'https://api.inaturalist.org/v1/places/{id}' + {'id': [1,2,3], 'admin_level': [0,10]} => 'https://www.api.inaturalist.org/v1/places/1,2,3?admin_level=0,10'\n    \"\"\"\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        pv = ','.join(v)\n        if url.find(pp:=f'{{{p}}}') >= 0:\n            url = url.replace(pp, pv)\n        else:\n            s = '?' if url.find('?') < 0 else '&'\n            url += f'{s}{p}={pv}'\n    return url\n\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0):\n    \"\"\"Fetch and convert repsonse to JSON\"\"\"\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = req_headers_base.copy() # make a copy\n        req_headers['Authorization'] = jwt\n    if 'pyfetch' in globals():\n        response = await pyfetch(url, method=method, headers=req_headers)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers)\n        data = response.json()\n    print(f'Fetch complete: {method} {url}')\n    return data\n\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    \"\"\"GET total_results (count) from the API\"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data['total_results'])\n    print(f'Total records: {str(total_results)}')\n    return total_results\n\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    \"\"\"GET a single page of results from the API. Can be called directly but generally is intended to be called by get_results.\n    Additional parsing and additional filtering before and after parsing can happen here, too.\n    \"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get('results',[])\n    if pre_parse_filter_function:\n        results = list(filter(pre_parse_filter_function, results))\n    if parse_function:\n        results = parse_function(results)\n    if post_parse_filter_function:\n        results = list(filter(post_parse_filter_function, results))\n    return results\n\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    \"\"\"GET results from the API. When get_all_pages=True, get results over multiple pages using 1 of 2 methods:\n    1. When the endpoint definition includes a page_key field, group key items into batches of up to a max number of records per page / batch.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/taxa/{id}', 'page_key': 'id', 'max_per_page': 30 } and params = {'id': ['1','2','3',...,'60']}\n       Then: GET https://api.inaturalist.org/v1/taxa/1,2,3,...,30; GET https://api.inaturalist.org/v1/taxa/31,32,33,...,60\n    2. In other cases, get pages with the max records per page, up to the maximum record limit that the API endpoint provides.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/observations', 'max_records': 10000, 'max_per_page': 200 } and params = {'taxon_id': ['1']}\n       Then: GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=1; GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=2; etc...\n    Get pages in parallel, with each page request having an incrementally delayed start. (iNaturalist suggests limiting requests to ~1 req/second.)\n    \"\"\"\n    if params is None:\n        params = {}\n    results = []\n    if (page_key := endpoint.get('page_key')):\n        if not (page_key_values := params.get(page_key)):\n            print(f'Cannot query from this endpoint without values for {page_key} parameter')\n            return None\n        # if more values are input than the max per page, split these into multiple batches\n        max_per_page = endpoint['max_per_page']\n        total_key_values = len(page_key_values)\n        batches = [page_key_values[i:i+max_per_page] for i in range(0, total_key_values, max_per_page)]\n        print(f'There are {total_key_values} {page_key} values, requiring {len(batches)} API requests to retrieve. Retrieving {\"all sets\" if get_all_pages else \"only the first set\"}...')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in (range(len(batches) if get_all_pages else 1)):\n                rp = params.copy() # make a copy\n                rp[page_key] = batches[i]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    else:\n        max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n        if get_all_pages:\n            # when getting all pages, make a small query first to find how many total records there are.\n            # this allows us to calculate how many requests we need to make in total.\n            # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n            total_results = await get_total_results(endpoint, params, use_authorization)\n            total_pages = math.ceil(total_results / endpoint['max_per_page'])\n            if total_pages < max_page:\n                max_page = total_pages\n            print(f'Pages to retrieve: {str(max_page)}')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in range(max_page):\n                rp = params.copy() # make a copy\n                if get_all_pages:\n                    # if getting all pages, remove per_page and page parameters if they exist in the base params\n                    # and then set per_page = max and increment page for each request\n                    rp.pop('per_page', None)\n                    rp.pop('page', None)\n                    rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                    rp['page'] = [str(i+1)]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    print(f'Total records retrieved: {str(len(results))}')\n    return results\n\ndef get_ref_value(rec, ref):\n    \"\"\"Used by another function get_field_value to get a particular value from a results row (rec).\n    If the reference is chained, then traverse through the chain to get to the final item (ex. ref = 'identifications.taxon.id' + rec => identifications => taxon => id).\n    If the chain includes list indexes (ex. index 0 in ref = 'photos[0].id'), then handle those situations, too.\n    \"\"\"\n    value = rec\n    dict_chain = ref.split('.')\n    for r in dict_chain:\n        list_chain = [];\n        if r.find('[') >= 0:\n            r = r.replace(']','')\n            r = r.split('[')\n            list_chain = r[1:len(r)]\n            r = r[0]\n        value = value.get(r)\n        if list_chain and value is not None:\n            for i in map(int, list_chain):\n                if len(value) == 0:\n                    value = None\n                    break\n                value = value[i]\n        if value is None:\n            break\n    return value\n\ndef filter_ref_value(rec, value, params):\n    \"\"\"Used by another function get_field_value to filter a nested list based on certain filter parameters\"\"\"\n    # filtered = [r for r in value if all([(get_ref_value(r,f.get('ref')) == (get_ref_value(rec, fvr) if (fvr := f.get('value_ref')) else f.get('value'))) for f in params])]\n    # the code below seems to run a tiny bit faster than the commented out line above\n    filtered = []\n    for r in value:\n        for f in params:\n            if not (get_ref_value(r,f.get('ref')) == (get_ref_value(rec, fvr) if (fvr := f.get('value_ref')) else f.get('value'))):\n                break\n        else:\n            filtered.append(r)\n    return filtered\n\ndef get_field_value(rec, field):\n    \"\"\"used by another function parse_results to parse a results row (rec) and get / calculate the value based on a field definition\"\"\"\n    # get the base value\n    value = get_ref_value(rec, field['ref']) if field['ref'] else rec \n    if value is None and field.get('alt'):\n        value = get_ref_value(rec, field['alt'])\n    # apply a transformation / calculation based on a specified \"function\" + \"params\" in the field definition\n    if (ff := field.get('function')) == 'count':\n        value = len(value) if value else 0\n    elif value is not None:\n        fp = field.get('params',{})\n        if ff == 'split':\n            value = value.split(fp.get('separator'))[fp.get('index')]\n        elif ff == 'join':\n            value = value = fp.get('separator').join(map(str, value)) if value else None\n        elif ff == 'replace':\n            value = value.replace(fp.get('old_text'), fp.get('new_text'))\n        elif ff == 'combine':\n            cvalue = fp.get('template','')\n            cref = fp.get('combine_refs',[])\n            for i, cr in enumerate(cref):\n                cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(value,cr)))\n            value = cvalue\n        elif ff == 'filter_combine':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = []\n            for r in filtered:\n                cvalue = fp.get('template','')\n                cref = fp.get('combine_refs',[])\n                for i, cr in enumerate(cref):\n                    cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(r,cr)))\n                fvalue.append(cvalue)\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n            if value == []:\n                value = None\n        elif ff == 'filter_count':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            value = len(filtered) if (dr := fp.get('distinct_ref')) is None else len(set([get_ref_value(r, dr) for r in filtered])) # get a distinct count if distinct_ref is defined\n        elif ff == 'filter_select':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = [get_ref_value(r, fp.get('select_ref')) for r in filtered]\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n    # transform based on a custom function\n    if (cf := field.get('custom_function')):\n        value = cf(value)\n    # cast the final value to a particular type specified in the field defintiion\n    if (cast_as := field.get('type')):\n        try:\n            value = cast_as(value)\n        except:\n            pass\n    return value\n\ndef parse_simple(results, field_list):\n    \"\"\"Return only specific fields from the results. Only top-level items may be specified in the field list, but children of selected items will be included with the returned values.\"\"\"\n    return [{k: r.get(k) for k in field_list} for r in results]\n\ndef parse_results(results, parse_fields, pre_parse_functions=None):\n    \"\"\"Parse a set of results, based on parse_fields defintiions. Pre-parse functions can be used to manipulate the results prior to parsing.\"\"\"\n    if pre_parse_functions is None:\n        pre_parse_functions = []\n    # parse based on the parse_fields defintion\n    presults = []\n    for r in results:\n        # special processing prior to core processing\n        for ppf in pre_parse_functions:\n            ppf(r)\n        # core processing\n        row = {}\n        for i, f in enumerate(parse_fields):\n            row[f.get('label') or f.get('ref') or f'col_{i+1}'] = get_field_value(r,f)\n        presults.append(row)\n    return presults\n\ndef refs_in_parse_fields(parse_fields):\n    \"\"\"Return a distinct list of field references from a list of field definitions (parse_fields).\n    This may be useful when v2 of the API allows the user to specify which fields to return in the API response.\n    \"\"\"\n    refs = []\n    for f in parse_fields:\n        function = f.get('function')\n        if (ref := f['ref']) is not None and (function is None or function in ['join','split','replace']):\n            refs.append(ref)\n        if (alt := f.get('alt')) is not None and (function is None or function in ['join','split','replace']):\n            refs.append(alt)\n        if (params := f.get('params')) is not None:\n            if (select_ref := params.get('select_ref')) is not None:\n                select_ref = f'{ref}.{select_ref}'\n                refs.append(ref)\n            if (distinct_ref := params.get('distinct_ref')) is not None:\n                distinct_ref = f'{ref}.{distinct_ref}'\n                refs.append(distinct_ref)\n            if (combine_refs := params.get('combine_refs')) is not None:\n                for cr in combine_refs:\n                    if ref is not None:\n                        cr = f'{ref}.{cr}'\n                    refs.append(cr)\n            if (filter_param := params.get('filter')) is not None:\n                for fp in filter_param:\n                    if (filter_ref := fp.get('ref')) is not None:\n                        if ref is not None:\n                            filter_ref = f'{ref}.{filter_ref}'\n                        refs.append(filter_ref)\n                    if (filter_value_ref := fp.get('value_ref')) is not None:\n                        if ref is not None:\n                            filter_value_ref = f'{ref}.{filter_value_ref}'\n                        refs.append(filter_value_ref)\n    for i, r in enumerate(refs): # remove list item indexes\n        while (pos_beg := r.find('[')) >= 0 and (pos_end := r.find(']')) >= 0:\n            r = r[:pos_beg] + r[pos_end+1:]\n            refs[i] = r\n    refs = list(set(refs)) # make distinct\n    refs.sort()\n    return refs\n\nasync def get_annotations():\n    \"\"\"Get annotation codes (controlled_term.ids, aka controlled_attribute_ids and controlled_value_ids) and descriptions (controlled_term.labels) from the API.\n    Then create a code / description cross-reference and store it in an attribute on the function named xref. \n    (Only the codes are included in the GET /v1/observations results. So the cross-reference is needed to translate the codes to plain English.)\n    \"\"\"\n    xref = getattr(get_annotations, 'xref', None)\n    if xref is None:\n        xref = {};\n        terms = await(fetch_data(endpoint_get_controlled_terms['url']))\n        for t in terms['results']:\n            xref[t['id']] = t['label']\n            for v in t['values']:\n               xref[v['id']] = v['label']\n        print(f'Retrieved annnotation cross-references ({len(xref)} items)')\n        get_annotations.xref = xref\n    return xref\n\ndef add_annot_descr_and_total_score(r):\n    \"\"\"Intended to be used as pre-parse function in parse_results when parsing observations.\n    Add annotation descriptions from get_annotations to a set of observation results. (Assumes get_annotations has been previously run.)\n    Also add total score equal to the existing vote_score + 1 (since the annotation itself should be considered a vote).\n    \"\"\"\n    for a in r.get('annotations',[]):\n        a['controlled_attribute'] = get_annotations.xref[a['controlled_attribute_id']]\n        a['controlled_value'] = get_annotations.xref[a['controlled_value_id']]\n        a['total_score'] = (1 + (vote_score if (vote_score := a.get('vote_score')) else 0)) # note: it's possible a single user can annotate and upvote, but there will be no special handling to eliminate such upvotes\n\ndef add_obs_field_taxon_or_value(r):\n    \"\"\"Intended to be used as pre-parse function in parse_results when parsing observations.\n    Add a field with either taxon name + id (when obs field value is a taxon) or a plain value (when obs field value is any other kind of value)\n    \"\"\"\n    for of in r.get('ofvs',[]):\n        of['taxon_or_value'] = f'{of[\"taxon\"][\"name\"]} ({of[\"taxon\"][\"id\"]})' if of['datatype'] == 'taxon' and of.get('taxon') else of['value']\n\ndef add_ident_vs_obs_comparison(r):\n    \"\"\"Intended to be used as pre-parse function in parse_results when parsing observations.\n    Add a field \"vs_obs\" which compares an identification's taxon vs the observation taxon (vs_obs = 'same', 'ancestor', 'descendant', 'different', or 'none')\n    \"\"\"\n    #ic = 0\n    for i, id in enumerate(r.get('identifications',[])):\n        #id['seq'] = i+1\n        #if id['current'] == 'true':\n        #    ic += 1\n        #    id['seq_current'] = ic\n        if not (rt := r.get('taxon')) or not (idt := id.get('taxon')):\n            id['vs_obs'] = 'none'\n        elif rt['id'] == idt['id']:\n            id['vs_obs'] = 'same'\n        elif (idta := idt.get('ancestry')) is not None and rt['id'] in map(int, idta.split('/')):\n            id['vs_obs'] = 'descendant'\n        elif (rta := rt.get('ancestry')) is not None and idt['id'] in map(int, rta.split('/')):\n            id['vs_obs'] = 'ancestor'\n        else:\n            id['vs_obs'] = 'different'\n\ndef add_obs_taxon_ancestors(r):\n    \"\"\"Intended to be used as pre-parse function in parse_results when parsing observations.\n    The observation taxon itself has an ancestor list but no detailed ancestor information; however, the taxon fields in the identiifcations do have ancestor details.\n    So this adds ancestor details to the observation taxon, based on the ancestor details in the identifications (since the observation taxon should always be included in the indentification taxa or their ancestors).\n    \"\"\"\n    ancestors = []\n    rank_level_kingdom = 70 # this is the highest-level taxon stored in identification[i].ancestors\n    if (rt := r.get('taxon')) and (taxon_id := rt.get('id')) is not None and (rank_level := rt.get('rank_level')) < rank_level_kingdom:\n        for id in r.get('identifications',[]):\n            if (idt := id.get('taxon')):\n                if idt['id'] == taxon_id:\n                    ancestors = list(idt['ancestors'])\n                    break\n                if (idta := idt['ancestors']):\n                    for i, atid in enumerate([a['id'] for a in idta]):\n                        if atid == taxon_id:\n                            ancestors = idta[0:i] # add everything above this taxon (will add this taxon later below)\n                            break\n                if ancestors:\n                    break\n    if rt and rank_level <= rank_level_kingdom:\n        ancestors.append(rt.copy())\n        rt['ancestors'] = ancestors\n\nasync def get_obs(params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    \"\"\"Get and parse observations from the API\"\"\"\n    if params is None:\n        params = {}\n    pre_parse_functions = []\n    post_get_functions = []\n    if parse_function is None:\n        if params.get('only_id',['false']) == ['true']: # if only_id=true, then don't parse fields because only id will exist in the results\n            parse_function = None\n        else: # if a custom parse_function is not specified, use parse_results with some default field definitions\n            # each dict in the field definition must have at least a ref (reference) key. (note: if ref is set to None, the observation row will be retrieved as the value.)\n            # use an optional label if you want the key to be different from the ref.\n            # use an optional type to cast the field to a specific data type.\n            # use an optional alt (alternative reference) if you want a fallback ref in case no data is found in ref.\n            # use optional function + params to do more complicated parsing of the ref,\n            # even more complicated logic can be handled with a custom_function, pre_parse_functions, or post_get_functions. \n            parse_fields = [\n                {'ref': 'id'},\n                #{'label': 'url', 'ref': None, 'function': 'combine', 'params': {'combine_refs': ['id'], 'template': 'https://www.inaturalist.org/observations/{0}'}},\n                #{'ref': 'uuid'},\n                {'ref': 'quality_grade'},\n                #{'label': 'user_id', 'ref': 'user.id'},\n                {'label': 'user_login', 'ref': 'user.login'},\n                #{'label': 'user_login_id', 'ref': 'user', 'function': 'combine', 'params': {'combine_refs': ['login','id'], 'template': '{0} ({1})'}},\n                #{'label': 'user_name', 'ref': 'user.name'},\n                #{'label': 'taxon_ancestors', 'ref': 'taxon.ancestors', 'function': 'filter_combine', 'params': {'combine_refs': ['name','rank','id'], 'template': '{0} ({1}) ({2})', 'separator': ', '}},\n                #{'label': 'kingdom', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'kingdom'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'phylum', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'phylum'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'class', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'class'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'order', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'order'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'family', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'family'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'genus', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'genus'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'species', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'species'}], 'select_ref': 'name', 'separator': ', '}},\n                {'label': 'taxon_id', 'ref': 'taxon.id'},\n                {'label': 'taxon_name', 'ref': 'taxon.name'},\n                {'label': 'taxon_preferred_common_name', 'ref': 'taxon.preferred_common_name'},\n                {'label': 'taxon_rank', 'ref': 'taxon.rank'},\n                #{'label': 'taxon_rank_level', 'ref': 'taxon.rank_level'},\n                #{'label': 'taxon_ancestry', 'ref': 'taxon.ancestry'},\n                #{'ref': 'observed_on_string'},\n                {'ref': 'time_observed_at'},\n                {'ref': 'created_at'},\n                #{'ref': 'updated_at'},\n                {'ref': 'place_guess'},\n                #{'ref': 'location'},\n                {'label': 'latitude', 'ref': 'location', 'type': float, 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n                {'label': 'longitude', 'ref': 'location', 'type': float, 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n                {'ref': 'public_positional_accuracy'},\n                #{'ref': 'private_place_guess'},\n                #{'ref': 'private_location'},\n                #{'label': 'private_latitude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n                #{'label': 'private_longitiude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n                #{'ref': 'positional_accuracy'},\n                {'ref': 'taxon_geoprivacy'},\n                {'ref': 'privacy'},\n                {'ref': 'description'},\n                {'label': 'photos_count', 'ref':'photos', 'function': 'count'},\n                #{'label': 'photo_1_id', 'ref': 'photos[0].id'},\n                {'label': 'photo_1_url', 'ref': 'photos[0].url', 'function': 'replace', 'params': {'old_text': 'square', 'new_text': 'medium'}}, # size options are thumb, square, small, medium, large, and original\n                {'label': 'photo_1_license_code', 'ref': 'photos[0].license_code'},\n                {'label': 'sounds_count', 'ref':'sounds', 'function': 'count'},\n                {'ref': 'comments_count'},\n                #{'label': 'others_current_identifications_count', 'ref': 'identifications_count'},\n                {'label': 'current_identifications_count', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}]}},\n                #{'label': 'current_identifications_by_observer', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}]}},\n                #{'label': 'current_identification_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}], 'select_ref': 'taxon.name', 'separator': ', '}},\n                #{'label': 'current_identification_category_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}], 'select_ref': 'category', 'separator': ', '}},\n                #{'ref': 'owners_identification_from_vision'},\n                {'label': 'prefers_community_taxon', 'ref': 'preferences.prefers_community_taxon', 'alt': 'user.preferences.prefers_community_taxa'},\n                #{'label': 'identifier_ids', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.id', 'separator': ', '}},\n                #{'label': 'identifier_logins', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.login', 'separator': ', '}},\n                {'label': 'identifications', 'ref': 'identifications', 'function': 'filter_combine', 'params': {'filter': [{'ref': 'current', 'value': True}], 'combine_refs': ['user.login','taxon.name','taxon.id'], 'template': '{0}: {1} ({2})', 'separator': ', '}},\n                #{'label': 'days_to_first_id', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(first_id_date) - datetime.fromisoformat(x['created_at'])).days if (first_id_date := get_ref_value(x,'identifications[0].created_at')) is not None else None)},\n                {'label': 'days_to_first_id_by_observer', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(dates_of_ids_by_observer[0]) - datetime.fromisoformat(x['created_at'])).days if (dates_of_ids_by_observer := [xi['created_at'] for xi in x.get('identifications') if (xi['user']['id']==x['user']['id'])]) else None)},\n                {'label': 'days_to_first_id_by_others', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(dates_of_ids_by_others[0]) - datetime.fromisoformat(x['created_at'])).days if (dates_of_ids_by_others := [xi['created_at'] for xi in x.get('identifications') if (xi['user']['id']!=x['user']['id'])]) else None)},\n                #{'label': 'identification_date_first', 'ref': 'identifications[0].created_at'},\n                #{'label': 'identification_date_last', 'ref': 'identifications[-1].created_at'},\n                #{'label': 'identifications_vs_obs', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'vs_obs', 'separator': ', '}},\n                {'label': 'identifications_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'same'}, {'ref': 'current', 'value': True}]}},\n                #{'label': 'ident_taxa_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'same'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'ancestor'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'ancestor'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'descendant'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'descendant'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'different'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'different'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                {'label': 'ident_disagreement_vs_obs', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'disagreement', 'value': True}, {'ref': 'current', 'value': True}], 'select_ref': 'vs_obs', 'separator': ', '}},\n                {'label': 'reviewed_by_count', 'ref': 'reviewed_by', 'function': 'count'},\n                #{'ref': 'reviewed_by', 'function': 'join', 'params': {'separator':', '}},\n                #{'ref': 'captive'},\n                {'label': 'annotations_count','ref':'annotations', 'function': 'count'},\n                #{'label': 'annotations_ids', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute_id','controlled_value_id'], 'template': '{0}:{1}', 'separator': ', '}},\n                {'label': 'annotations', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute','controlled_value'], 'template': '{0}: {1}', 'separator': ', '}}, # note: this relies on some pre-procesing to create the controlled_attribute and controlled_value fields\n                #{'label': 'annotations_sex', 'ref': 'annotations', 'function': 'filter_count', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 9}]}},\n                #{'label': 'annot_score_sex_female', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 9}, {'ref': 'controlled_value_id', 'value': 10}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_sex_male', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 9}, {'ref': 'controlled_value_id', 'value': 11}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_sex_cannot_det', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 9}, {'ref': 'controlled_value_id', 'value': 20}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annotations_phenology', 'ref': 'annotations', 'function': 'filter_count', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 12}]}},\n                #{'label': 'annot_score_phen_flowering', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 12}, {'ref': 'controlled_value_id', 'value': 13}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_phen_fruiting', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 12}, {'ref': 'controlled_value_id', 'value': 14}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_phen_flower_bud', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 12}, {'ref': 'controlled_value_id', 'value': 15}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_phen_no_evid_of_flower', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 12}, {'ref': 'controlled_value_id', 'value': 21}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annotations_life_stage', 'ref': 'annotations', 'function': 'filter_count', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}]}},\n                #{'label': 'annot_score_stage_adult', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 2}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_teneral', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 3}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_pupa', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 4}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_nymph', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 5}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_larva', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 6}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_egg', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 7}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_juvenile', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 8}], 'select_ref': 'total_score', 'separator': ', '}},\n                #{'label': 'annot_score_stage_subimago', 'ref': 'annotations', 'function': 'filter_select', 'params': {'filter': [{'ref': 'controlled_attribute_id', 'value': 1}, {'ref': 'controlled_value_id', 'value': 16}], 'select_ref': 'total_score', 'separator': ', '}},\n                {'label': 'observation_fields_count', 'ref':'ofvs', 'function': 'count'},\n                #{'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','value'], 'template': '{0} ({1}): {2}', 'separator': '; '}},\n                {'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','taxon_or_value'], 'template': '{0} ({1}): {2}', 'separator': '; '}}, # note: this relies on pre-procesing to create a field that contains either taxon or value\n                {'label': 'tags_count', 'ref':'tags', 'function': 'count'},\n                {'ref': 'tags', 'function': 'join', 'params': {'separator': ', '}},\n                #{'ref': 'oauth_application_id'},\n                #{'ref': 'site_id'},\n                {'label': 'gbif_occurence_url', 'ref': 'outlinks', 'function': 'filter_select', 'params': {'filter': [{'ref': 'source', 'value': 'GBIF'}], 'select_ref': 'url', 'separator': ', '}},\n                #{'ref': 'place_ids'}, # if this is retrieved, this will be replaced with standard place information (later via a post-get function)\n            ]\n            parse_field_refs = refs_in_parse_fields(parse_fields)\n            # use pre-parse functions to handle more complicated stiuations which can be handled per record / page\n            pre_parse_function_associations = [\n                {'function': add_annot_descr_and_total_score, 'new_refs': ['annotations.controlled_attribute', 'annotations.controlled_value','annotations.total_score'], 'prereq_function': get_annotations},\n                {'function': add_obs_taxon_ancestors, 'new_refs': ['taxon.ancestors']},\n                {'function': add_obs_field_taxon_or_value, 'new_refs': ['ofvs.taxon_or_value']},\n                {'function': add_ident_vs_obs_comparison, 'new_refs': ['identifications.vs_obs']},\n            ]\n            for ppfa in pre_parse_function_associations: # run certain pre-parse_functions in cases where parse_fields defintiions reference fields created by these functions\n                if len([nr for nr in ppfa['new_refs'] if len([ppr for ppr in parse_field_refs if ppr.startswith(nr)]) > 0]) > 0:\n                    if (prereq_function := ppfa.get('prereq_function')):\n                        await prereq_function()\n                    pre_parse_functions.append(ppfa['function'])\n            # use post-get functions to handle more complicated situations which are best handled once the entire set of observations is available\n            if 'place_ids' in parse_field_refs:\n                post_get_functions.append(add_std_places)\n            parse_function = partial(parse_results, parse_fields=parse_fields, pre_parse_functions=pre_parse_functions) # pre-load parse_fields with these parse_fields and pre_parse_functions\n    print('Getting observations...')\n    results = await get_results(endpoint_get_obs, params, get_all_pages, use_authorization, parse_function, pre_parse_filter_function, post_parse_filter_function)\n    for pgf in post_get_functions:\n        try:\n            await pgf(results) # assume async function by default\n        except:\n            pgf(results) # fall back to regular execution for non-async functions\n    return results\n\nasync def add_std_places(obs, remove_place_ids=True):\n    \"\"\"Intended to be run as a post-get function in parse_obs (after getting all pages of observations, to minimize the number of requests to get place data).\n    Add human-friendly standard place information to observations, if places_ids are included in (parsed) results\n    \"\"\"\n    # get a unique list of place_ids associated with the observations\n    place_ids = [pid if (pid := o.get('place_ids')) else [] for o in obs]\n    place_ids = set([pp for p in place_ids for pp in p])\n    if not place_ids:\n        return obs\n    print('Adding standard place info to observations...')\n    # request info from the API for only the \"standard\" places (continents, plus country-, state-, county-, and town-equivalent places)\n    admin_level_xref = { # define these in the order these should be displayed in the results\n        #'30': 'town', # these are available only for certain states in the USA\n        '20':'county',\n        '10':'state',\n        '0':'country',\n        '-10': 'continent',\n    }\n    place_req_params = {'id': list(map(str,place_ids)), 'admin_level': list(admin_level_xref.keys())}\n    std_places = await get_results(endpoint_get_places, place_req_params, get_all_pages=True, parse_function=partial(parse_simple, field_list=['id','admin_level','name','slug']))\n    if std_places is None:\n        std_places = []\n    # add std place info to observations\n    for o in obs:\n        osp = [sp for sp in std_places if sp['id'] in op] if (op := o.get('place_ids')) else [] # find standard places associated with this obs\n        for alc, ald in admin_level_xref.items():\n            alp = [ospp for ospp in osp if ospp['admin_level'] == int(alc)] # find the place with this admin level\n            o[f'std_place_{ald}'] = alp[0]['name'] if alp else None\n        if remove_place_ids: \n            o.pop('place_ids',None) # remove this column from the parsed results, since it's no longer needed\n    return obs\n\nasync def get_count_series(endpoint, series, series_params, base_params=None, count_label='rec_count', use_authorization=False, add_count_to_series=False):\n    \"\"\"Get a series of counts\n    Base_params are the (fixed) parameters that will be applied when getting the count for each item in the series.\n    Series_params is a list of (variable) parameters (keys) to add to base_params for each item in the series.\n    Series is a list of dicts, each of which defines the parameter key/value pairs for each item in the series.\n    Each item in the series list can contain additional attributes that are not parameters, and it does not have to contain all the keys in the series_params list.\n    If add_count_to_series is set to True, the function will add the counts to the original series object; otherwise, it just returns a (somewhat deep) copy of series with counts.\n    \"\"\"\n    if base_params is None:\n        base_params = {}\n    if not series or not series_params:\n        print(f'The series parameter must be a list of dicts with keys that include the values in the list passed in for series_params.')\n        return None\n    rv = []\n    results = series if add_count_to_series else series.copy() # return values will look the same, but if add_count_to_series=True, the original series list wlll actually change\n    async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n        tasks = []\n        for i, r in enumerate(results):\n            rp = base_params.copy()\n            for sp in series_params:\n                if (spv := r.get(sp)) is not None:\n                    rp.pop(sp, None)\n                    rp[sp] = [str(spv)]\n            tasks.append(tg.create_task(get_total_results(endpoint, rp, use_authorization=use_authorization, delay=i)))\n    for i, t in enumerate(tasks):\n        if not add_count_to_series:\n            results[i] = results[i].copy() # return values will look the same, but if add_count_to_series=True, the original series list wlll actually change\n        results[i][count_label] = t.result()\n    return results\n\ndef items_to_batches(items, max_batch_size=500, separator=',', prefix=''):\n    \"\"\"String together a list of items into batches of up to a max number of items per set.\n    (The original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for batches of specific observations.)\n    \"\"\"\n    batches = []\n    for i in range(0, len(items), max_batch_size):\n        items_string = prefix + separator.join(map(str, items[i:i+max_batch_size]))\n        batches.append(items_string)\n        print(f'Batch {int(i/max_batch_size+1)}: {items_string}')\n    return batches",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "raw_mimetype": "",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'verifiable=true&spam=false'\nreq_params = params_to_dict(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_obs = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/observations',\n    'max_records': 10000,\n    'max_per_page': 200,\n}\nendpoint_get_controlled_terms = {\n    'method': 'GET' ,\n    'url': 'https://api.inaturalist.org/v1/controlled_terms',\n}\nendpoint_get_places = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/places/{id}',\n    'max_per_page': 500,\n    'page_key': 'id'\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section\n\n# get observations\nobs = await get_obs(req_params, get_all_pages=False, use_authorization=False)\n#obs\n\n# when possible, it's always best to filter on the server side by using filter parameters when making API requests.\n# but when a particular filter is not available in the API, it may still be possible to filter on the client side (as opposed to server side)\n# use pre_parse_filter_function when you can filter based on the results directly from the API response.\n# use post_parse_filter_function when you must rely on the values in a parsed field to do the filtering.\n# (you can always filter separately *after* getting observations, of course, but filtering *while* getting obs saves on system memory when getting multiple pages of results from the API.)\n# here's an example of how to do client-side filtering for observations which have >1 (current) identification using post_parse_filter_function\n#obs = await get_obs(req_params, get_all_pages=False, use_authorization=False, post_parse_filter_function=(lambda x: x['current_identifications_count'] > 1))\n\n# get observation ids from obs\n#obs_ids = [o.get('id') for o in obs]\n#obs_id_sets = items_to_batches(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')\n\n# get just total results (count)\n#obs_count = await get_total_results(endpoint_get_obs, req_params, use_authorization=False)\n#obs_count\n\n# get a series of counts\n#obs_count_series = [\n#    {'label': 'Texas 2020', 'year': 2020, 'place_id': 18},\n#    {'label': 'not Texas 2020', 'year': 2020, 'not_in_place': 18},\n#    {'label': 'Texas 2021', 'year': 2021, 'place_id': 18},\n#    {'label': 'not Texas 2021', 'year': 2021, 'not_in_place': 18},\n#]\n#await get_count_series(endpoint_get_obs, obs_count_series, ['year','place_id','not_in_place'], base_params=req_params, count_label='obs_count', use_authorization=False, add_count_to_series=True)\n#obs_count_series",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# if you order by id when you get observations (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\n# to use the code below, set get_more_obs = True before running.\nget_more_obs = False\n#if get_more_obs and obs and len(obs) >= endpoint_get_obs['max_records'] and len(obs) % endpoint_get_obs['max_records'] == 0:\nif get_more_obs and obs:\n    rp = req_params.copy() # make a copy\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([o.get('id') for o in obs])\n            print(f'Getting additional observations for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the obs we already have\n        else:\n            min_id = min([o.get('id') for o in obs])\n            print(f'Getting additional observations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the obs we already have\n        obs += await get_obs(rp, get_all_pages=True, use_authorization=False)\n        print(f'Observations accumulated: {len(obs)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Write Data to CSV",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Ths takes the results retrieved above and writes them to a CSV file. The file will appear in the main folder of the file tree (the topmost tab in the left pane of the JupyterLab interface). Files generated in JupyterLite are saved to the browser's storage. So those will need to be downloaded to a more permanent location if they need to be archived more permanently.",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport csv # used to output CSV files",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def data_to_csv(data, csv_filename='export.csv'):\n    \"\"\"Write data to a CSV file\"\"\"\n    csv_fields = list(data[0]) # get fields from the keys of the first record in the dataset\n    with open(csv_filename, 'w', newline='') as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n        csv_writer.writeheader()\n        csv_writer.writerows(data)\n        print(f'Created CSV file {csv_filename} with {len(data)} records.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# export to CSV\ndata_to_csv(obs,'observations.csv')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Work with Data in a DataFrame",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since many Python analysis / visualization modules and workflows rely on getting data into a `pandas` dataframe, this provides a very barebones example of getting the data into a dataframe. The dataframe should generally handle most of the data type conversions, but there's a little bit more effort to get dates into a datetime typed column in the dataframe.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# load data into a DataFrame (df)\ndf = pd.DataFrame(obs)\n\n# pandas can export to CSV, too\n#df.to_csv('observations_from_df.csv', index=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Preview the contents of the df\ndf\n\n# miscellaneous other examples:\n\n# first 10 records\n#df[0:9]\n\n# records where observation_fields are not null\n#df.loc[df.observation_fields.notnull()]\n\n# count (of id of) records where acc > 100\n#df.loc[df.public_positional_accuracy > 100].id.count()\n\n# id, lat, and long, sorted by latitude\n#df[['id','latitude','longitude']].sort_values('latitude', ascending=True)\n\n# count by taxon rank, and sort by count descending\n#df.groupby('taxon_rank').id.count().sort_values(ascending=False)",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Get basic summary statistics for df\ndf.describe()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# convert datetime columns to datetimes, localized to UTC\nfor k in ['time_observed_at','created_at','updated_at']:\n    if k in df.columns:\n        try:\n            df[k] = pd.to_datetime(df[k], utc=True, errors='coerce')\n        except:\n            print(f'Could not convert column {k} to datetime')\n\n# get count (of id) by observed year\ndf.groupby(df.time_observed_at.dt.year).id.count()\n\n# get count (of id) by created year\n#df.groupby(df.created_at.dt.year).id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}