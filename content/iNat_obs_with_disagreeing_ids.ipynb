{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API Example: Finding observations with disagreeing IDs\n- Link: https://jumear.github.io/stirpy/lab?path=iNat_obs_with_disagreeing_ids.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "In the [iNatForum](https://forum.inaturalist.org/), folks often ask how to filter for observations with disagreements, and there is even a [Feature Request](https://forum.inaturalist.org/t/provide-a-way-to-filter-observations-by-disputed-ids/6698) to implement a basic form of this kind of filter.\n\nUnder the hood, the [Explore](https://www.inaturalist.org/observations) and [Identify](https://www.inaturalist.org/observations/identify) pages get results from the [`GET /v1/observations`](https://api.inaturalist.org/v1/docs/#!/Observations/get_observations) API endpoint. Although that endpoint provides a `identifications[i].disagreement` field in its response, which indicates whether or not an observation's identification is a disagreement, there is not a filter parameter that can be used to return only observations which have an identifications where `identifications[i].disagreement=true`.\n\nIf we move away from observations, it is possible to use [`GET /v1/identifications`](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) to find *identifications* where `disagreement=true`. The problem is that there are no user interfaces in the system which display identifications data from this endpoint in a human-friendly way (although there is at least one [third-party tool](https://jumear.github.io/stirfry/iNatAPIv1_identifications) that can fill this gap). However, it is possible to get the observation IDs from those identifcation records, and then display those observations by passing an `id=[comma separated list of observation IDs]` parameter to the Explore and Identify pages. So this script provides an example of how to do that in a somewhat automated way.\n\nOne limitation of this workflow is that the GET /v1/identifications endpoint provides fewer [available filter parameters](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) than the GET /v1/observations endpoint. For example, it is possible to filter by project when filtering for observations, but it is not possible to filter for identifications by project. However, since we are effectively going through GET /v1/observations in the end, we can apply those additional observation filter parameters on top then.\n\nAnother limitation is that identification records don't seem to have recorded `disagreement=true` prior to 2018-01-03. So if the disagreement occurred prior to then, it will not be picked up by this workflow. A similar limitation is that it is possible to withdraw or replace an initial identification where `disagreement=true` in a way that subsequent identifications will not be recorded with `disagreement=true` and so cannot be picked up by this workflow. Finding disagreements in either of these cases requires a more inefficient client-side filtering of observation records which will not be covered in this example.\n\nAlthough this notebook was created with the intent of finding observations with disagreements, the basic concept of getting ids and then displaying the observations associated with those IDs can be applied to other purposes as well (ex. filtering for observations where a specific user made an identification of a specific taxon).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\n#from datetime import datetime # used to convert string datetimes into actual datetimes\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\n    use_pyfetch=True\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().\n    use_pyfetch=False",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define custom functions used for getting data\n\n# function to turn a parameter string into a dict\ndef params_to_dict(params_string):\n    params_dict = parse_qs(params_string)\n    for p, v in params_dict.items():\n        if v: # iNaturalist handles multiple values for the same parameter using comma separated values. since parse_qs doesn't handle that situation, this section will handle it.\n            v = [(vv.split(',') if vv else vv) for vv in v]\n            params_dict[p] = [vvv for vv in v for vvv in vv]\n    return params_dict\n\n# function to combine a base url with a set of parameters. (there's a urlencode method in urllib.parse, but it's easier to get exactly what I need using this custom code.)\n# iNaturalist parameters are sometimes passed through the endpoint URL path rather than through the query string. so this handles that specific case.\ndef url_with_params(url_base, params=None):\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        pv = ','.join(v)\n        if url.find(pp:=f'{{{p}}}') >= 0:\n            url = url.replace(pp, pv)\n        else:\n            s = '?' if url.find('?') < 0 else '&'\n            url += f'{s}{p}={pv}'\n    return url\n\n# basic function to fetch from API and convert repsonse to JSON\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0):\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = req_headers_base.copy() # make a copy\n        req_headers['Authorization'] = jwt\n    if use_pyfetch:\n        response = await pyfetch(url, method=method, headers=req_headers)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers)\n        data = response.json()\n    print(f'Fetch complete: {method} {url}')\n    return data\n\n# function to GET total_results (count) from the API\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data['total_results'])\n    print(f'Total records: {str(total_results)}')\n    return total_results\n\n# function to GET a single page of results from the API\n# additional parsing and additional filtering before and after the parsing can happen here, too\n# can be called directly but generally is intended to be called through get_results\n# note that the parse_function can be an async function (in case we need to get additional information from the API during parsing)\n# but if the parse_function gets additional data from the API for every page, then the delay used by get_results may need to be tweaked to keep within request limits\n# (ideally, cases where additional data wlll be needed from the API for every page should be handled after all pages have been retreieved)\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get('results',[])\n    if pre_parse_filter_function:\n        results = list(filter(pre_parse_filter_function, results))\n    if parse_function:\n        try:\n            results = await parse_function(results) # assume async function first\n        except:\n            results = parse_function(results) # fall back to non-async function \n    if post_parse_filter_function:\n        results = list(filter(post_parse_filter_function, results))\n    return results\n\n# function to GET results from the API\n# if get_all_pages=True, then get all records, up to the limit that the API endpoint provides.\n# query pages in parallel, with each page having a incrementally delayed start.\n# (iNaturalist wants you to limit requests to ~1 req/second.)\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    if params is None:\n        params = {}\n    results = []\n    if (page_key := endpoint.get('page_key')):\n        if not (page_key_values := params.get(page_key)):\n            print(f'Cannot query from this endpoint without values for {page_key} parameter')\n            return None\n        max_per_page = endpoint['max_per_page']\n        total_key_values = len(page_key_values)\n        page_sets = [page_key_values[i:i+max_per_page] for i in range(0, total_key_values, max_per_page)]\n        print(f'There are {total_key_values} {page_key} values, requiring {len(page_sets)} API requests to retrieve. Retrieving {\"all sets\" if get_all_pages else \"only the first set\"}...')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in (range(len(page_sets) if get_all_pages else 1)):\n                rp = params.copy() # make a copy\n                rp[page_key] = page_sets[i]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    else:\n        max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n        if get_all_pages:\n            # when getting all pages, make a small query first to find how many total records there are.\n            # this allows us to calculate how many requests we need to make in total.\n            # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n            total_results = await get_total_results(endpoint, params, use_authorization)\n            total_pages = math.ceil(total_results / endpoint['max_per_page'])\n            if total_pages < max_page:\n                max_page = total_pages\n            print(f'Pages to retrieve: {str(max_page)}')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in range(max_page):\n                rp = params.copy() # make a copy\n                if get_all_pages:\n                    # if getting all pages, remove per_page and page parameters if they exist in the base params\n                    # and then set per_page = max and increment page for each request\n                    rp.pop('per_page', None)\n                    rp.pop('page', None)\n                    rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                    rp['page'] = [str(i+1)]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    print(f'Total records retrieved: {str(len(results))}')\n    return results\n\n# function to string together a list of observation ids into sets of up to a max number of observations per set\n# the original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for specific observations\ndef obs_ids_to_sets(obs_ids, max_set_size=500, separator=',', prefix=''):\n    obs_id_sets = []\n    for i in range(0, len(obs_ids), max_set_size):\n        obs_id_string = prefix + separator.join(map(str, obs_ids[i:i+max_set_size]))\n        obs_id_sets.append(obs_id_string)\n        print(f'Set {int(i/max_set_size+1)}: {obs_id_string}')\n    return obs_id_sets",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'per_page=200&disagreement=true&place_id=110679' # remember: these are filter parameters for identifications, not observations.\nreq_params = parse_qs(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_ids = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/identifications',\n    'max_records': 10000,\n    'max_per_page': 200,\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 1\n\n# get identifications, filtered by the parameters defined in the request parameters (req_params)\nids = await get_results(endpoint_get_ids, req_params, get_all_pages=False, use_authorization=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 2\n\n# extract the observation ids associated with the identifications\nobs_ids = [oi['observation']['id'] for oi in ids]\n\n# string together the observation IDs, along with with a prefix, to create links to iNaturalist\n# these will be printed in the cell output below. click on the URLs in the output to open a browser tab/window to that URL.\nobs_id_sets = obs_ids_to_sets(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# optional execution section\n# this can be used to fetch and accumulate additional identifications after part 1 has already run, without having to change the main request parameters\n# to use the code below, set get_more_ids = True before running.\n\n# if you order by id when you get identifications (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\nget_more_ids = False\nif get_more_ids and ids:\n    rp = dict(req_params) # make a copy\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([i.get('id') for i in ids])\n            print(f'getting additional identifications for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the identifications we already have\n        else:\n            min_id = min([i.get('id') for i in ids])\n            print(f'getting additional identifcations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the identifications we already have\n        ids += await get_results(endpoint_get_ids, rp, get_all_pages=False, use_authorization=False)\n        print(f'identifications accumulated: {len(ids)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}