{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API Example: Finding observations with disagreeing IDs\n- Link: https://jumear.github.io/stirpy/lab?path=iNat_obs_with_disagreeing_ids.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "In the [iNatForum](https://forum.inaturalist.org/), folks often ask how to filter for observations with disagreements, and there is even a [Feature Request](https://forum.inaturalist.org/t/provide-a-way-to-filter-observations-by-disputed-ids/6698) to implement a basic form of this kind of filter.\n\nUnder the hood, the [Explore](https://www.inaturalist.org/observations) and [Identify](https://www.inaturalist.org/observations/identify) pages get results from the [`GET /v1/observations`](https://api.inaturalist.org/v1/docs/#!/Observations/get_observations) API endpoint. That endpoint provides a `identifications[i].disagreement` field in its response, which indicates whether or not an observation's identification is a disagreement, and there is a filter parameter `disagreements=true` that can be used to return only observations which have an identifications where `identifications[i].disagreement=true`. However, it's not possible to also filter on the server for additional information related to the disagreeing identification.\n\nIf we move away from observations, it is possible to use [`GET /v1/identifications`](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) to find *identifications* where `disagreement=true` (and also filter for additional information about the identiication, such as taxon, identifier, etc.). The problem is that there are no user interfaces in the system which display identifications data from this endpoint in a human-friendly way (although there is at least one [third-party tool](https://jumear.github.io/stirfry/iNatAPIv1_identifications) that can fill this gap). However, it is possible to get the observation IDs from those identifcation records, and then display those observations by passing an `id=[comma separated list of observation IDs]` parameter to the Explore and Identify pages. So this script provides an example of how to do that in a somewhat automated way.\n\nOne limitation of this workflow is that the GET /v1/identifications endpoint provides fewer [available filter parameters](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) than the GET /v1/observations endpoint. For example, it is possible to filter by project when filtering for observations, but it is not possible to filter for identifications by project. However, since we are effectively going through GET /v1/observations in the end, we can apply those additional observation filter parameters on top then.\n\nAnother limitation is that identification records don't seem to have recorded `disagreement=true` prior to 2018-01-03. So if the disagreement occurred prior to then, it will not be picked up by this workflow. A similar limitation is that it is possible to withdraw or replace an initial identification where `disagreement=true` in a way that subsequent identifications will not be recorded with `disagreement=true` and so cannot be picked up by this workflow. Finding disagreements in either of these cases requires a more inefficient client-side filtering of observation records which will not be covered in this example.\n\nAlthough this notebook was created with the intent of finding observations with disagreements, the basic concept of getting ids and then displaying the observations associated with those IDs can be applied to other purposes as well (ex. filtering for observations where a specific user made an identification of a specific taxon).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\nfrom functools import partial # used for pre-loading functions with some arguments\n#from datetime import datetime # used to convert string datetimes into actual datetimes\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define custom functions used for getting data\n\ndef params_to_dict(params_string):\n    \"\"\"Convert a parameter string into a dict.\n    ex.: 'taxon_id=1&user_id=kueda,loarie&field:Eating' => {'taxon_id': ['1'], 'user_id': ['kueda', 'loarie'], 'field:Eating': None}\n    \"\"\"\n    params_dict = {pkv[0]: pkv[1].split(',') if len(pkv) > 1 else None for pkv in [ps.split('=') for ps in params_string.split('&')]}\n    return params_dict\n\ndef url_with_params(url_base, params=None):\n    \"\"\"Combine a base url with a set of parameters. Can handle the following types of cases:\n    1. 'https://api.inaturalist.org/v1/observations' + {'taxon_id': [1], 'user_id': ['kueda','loarie']} => 'https://api.inaturlaist.org/v1/observations?taxon_id=1&user_id=kueda,loarie'\n    2. 'https://api.inaturalist.org/v1/places/{id}' + {'id': [1,2,3], 'admin_level': [0,10]} => 'https://www.api.inaturalist.org/v1/places/1,2,3?admin_level=0,10'\n    \"\"\"\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        pv = ','.join(v)\n        if url.find(pp:=f'{{{p}}}') >= 0:\n            url = url.replace(pp, pv)\n        else:\n            s = '?' if url.find('?') < 0 else '&'\n            url += f'{s}{p}={pv}'\n    return url\n\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0, body=None):\n    \"\"\"Fetch and convert repsonse to JSON\"\"\"\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = req_headers_base.copy() # make a copy\n        req_headers['Authorization'] = jwt\n    if 'pyfetch' in globals():\n        response = await pyfetch(url, method=method, headers=req_headers, body=body)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers, body=body)\n        data = response.json()\n    print(f'Fetch complete: {method} {url}')\n    return data\n\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    \"\"\"GET total_results (count) from the API\"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data[endpoint.get('record_count_field','total_results')])\n    print(f'Total records: {str(total_results)}')\n    return total_results\n\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    \"\"\"GET a single page of results from the API. Can be called directly but generally is intended to be called by get_results.\n    Additional parsing and additional filtering before and after parsing can happen here, too.\n    \"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get(endpoint.get('record_array','results'),[])\n    if pre_parse_filter_function:\n        results = list(filter(pre_parse_filter_function, results))\n    if parse_function:\n        results = parse_function(results)\n    if post_parse_filter_function:\n        results = list(filter(post_parse_filter_function, results))\n    return results\n\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    \"\"\"GET results from the API. When get_all_pages=True, get results over multiple pages using 1 of 2 methods:\n    1. When the endpoint definition includes a page_key field, group key items into batches of up to a max number of records per page / batch.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/taxa/{id}', 'page_key': 'id', 'max_per_page': 30 } and params = {'id': ['1','2','3',...,'60']}\n       Then: GET https://api.inaturalist.org/v1/taxa/1,2,3,...,30; GET https://api.inaturalist.org/v1/taxa/31,32,33,...,60\n    2. In other cases, get pages with the max records per page, up to the maximum record limit that the API endpoint provides.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/observations', 'max_records': 10000, 'max_per_page': 200 } and params = {'taxon_id': ['1']}\n       Then: GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=1; GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=2; etc...\n    Get pages in parallel, with each page request having an incrementally delayed start. (iNaturalist suggests limiting requests to ~1 req/second.)\n    \"\"\"\n    if params is None:\n        params = {}\n    results = []\n    if (page_key := endpoint.get('page_key')):\n        if not (page_key_values := params.get(page_key)):\n            print(f'Cannot query from this endpoint without values for {page_key} parameter')\n            return None\n        # if more values are input than the max per page, split these into multiple batches\n        max_per_page = endpoint['max_per_page']\n        total_key_values = len(page_key_values)\n        batches = [page_key_values[i:i+max_per_page] for i in range(0, total_key_values, max_per_page)]\n        print(f'There are {total_key_values} {page_key} values, requiring {len(batches)} API requests to retrieve. Retrieving {\"all sets\" if get_all_pages else \"only the first set\"}...')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in (range(len(batches) if get_all_pages else 1)):\n                rp = params.copy() # make a copy\n                rp[page_key] = batches[i]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    else:\n        max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n        if get_all_pages:\n            # when getting all pages, make a small query first to find how many total records there are.\n            # this allows us to calculate how many requests we need to make in total.\n            # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n            total_results = await get_total_results(endpoint, params, use_authorization)\n            total_pages = math.ceil(total_results / endpoint['max_per_page'])\n            if total_pages < max_page:\n                max_page = total_pages\n            print(f'Pages to retrieve: {str(max_page)}')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in range(max_page):\n                rp = params.copy() # make a copy\n                if get_all_pages:\n                    # if getting all pages, remove per_page and page parameters if they exist in the base params\n                    # and then set per_page = max and increment page for each request\n                    rp.pop('per_page', None)\n                    rp.pop('page', None)\n                    rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                    rp['page'] = [str(i+1)]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    print(f'Total records retrieved: {str(len(results))}')\n    return results\n\ndef items_to_batches(items, max_batch_size=500, separator=',', prefix=''):\n    \"\"\"String together a list of items into batches of up to a max number of items per set.\n    (The original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for batches of specific observations.)\n    \"\"\"\n    batches = []\n    for i in range(0, len(items), max_batch_size):\n        items_string = prefix + separator.join(map(str, items[i:i+max_batch_size]))\n        batches.append(items_string)\n        print(f'Batch {int(i/max_batch_size+1)}: {items_string}')\n    return batches",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'per_page=200&disagreement=true&place_id=110679' # remember: these are filter parameters for identifications, not observations.\nreq_params = params_to_dict(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_ids = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/identifications',\n    'max_records': 10000,\n    'max_per_page': 200,\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 1\n\n# get identifications, filtered by the parameters defined in the request parameters (req_params)\nids = await get_results(endpoint_get_ids, req_params, get_all_pages=False, use_authorization=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 2\n\n# extract the observation ids associated with the identifications\nobs_ids = [oi['observation']['id'] for oi in ids]\n\n# string together the observation IDs, along with with a prefix, to create links to iNaturalist\n# these will be printed in the cell output below. click on the URLs in the output to open a browser tab/window to that URL.\nobs_id_sets = items_to_batches(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# optional execution section\n# this can be used to fetch and accumulate additional identifications after part 1 has already run, without having to change the main request parameters\n# to use the code below, set get_more_ids = True before running.\n\n# if you order by id when you get identifications (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\nget_more_ids = False\nif get_more_ids and ids:\n    rp = dict(req_params) # make a copy\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([i.get('id') for i in ids])\n            print(f'getting additional identifications for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the identifications we already have\n        else:\n            min_id = min([i.get('id') for i in ids])\n            print(f'getting additional identifcations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the identifications we already have\n        ids += await get_results(endpoint_get_ids, rp, get_all_pages=False, use_authorization=False)\n        print(f'identifications accumulated: {len(ids)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}