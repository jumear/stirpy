{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API Example: Finding observations with disagreeing IDs\n- Link: https://jumear.github.io/stirpy/lab?path=iNat_obs_with_disagreeing_ids.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "In the [iNatForum](https://forum.inaturalist.org/), folks often ask how to filter for observations with disagreements, and there is even a [Feature Request](https://forum.inaturalist.org/t/provide-a-way-to-filter-observations-by-disputed-ids/6698) to implement a basic form of this kind of filter.\n\nUnder the hood, the [Explore](https://www.inaturalist.org/observations) and [Identify](https://www.inaturalist.org/observations/identify) pages get results from the [`GET /v1/observations`](https://api.inaturalist.org/v1/docs/#!/Observations/get_observations) API endpoint. Although that endpoint provides a `identifications[i].disagreement` field in its response, which indicates whether or not an observation's identification is a disagreement, there is not a filter parameter that can be used to return only observations which have an identifications where `identifications[i].disagreement=true`.\n\nIf we move away from observations, it is possible to use [`GET /v1/identifications`](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) to find *identifications* where `disagreement=true`. The problem is that there are no user interfaces in the system which display identifications data from this endpoint in a human-friendly way (although there is at least one [third-party tool](https://jumear.github.io/stirfry/iNatAPIv1_identifications) that can fill this gap). However, it is possible to get the observation IDs from those identifcation records, and then display those observations by passing an `id=[comma separated list of observation IDs]` parameter to the Explore and Identify pages. So this script provides an example of how to do that in a somewhat automated way.\n\nOne limitation of this workflow is that the GET /v1/identifications endpoint provides fewer [available filter parameters](https://api.inaturalist.org/v1/docs/#!/Identifications/get_identifications) than the GET /v1/observations endpoint. For example, it is possible to filter by project when filtering for observations, but it is not possible to filter for identifications by project. However, since we are effectively going through GET /v1/observations in the end, we can apply those additional observation filter parameters on top then.\n\nAnother limitation is that identification records don't seem to have recorded `disagreement=true` prior to 2018-01-03. So if the disagreement occurred prior to then, it will not be picked up by this workflow. A similar limitation is that it is possible to withdraw or replace an initial identification where `disagreement=true` in a way that subsequent identifications will not be recorded with `disagreement=true` and so cannot be picked up by this workflow. Finding disagreements in either of these cases requires a more inefficient client-side filtering of observation records which will not be covered in this example.\n\nAlthough this notebook was created with the intent of finding observations with disagreements, the basic concept of getting ids and then displaying the observations associated with those IDs can be applied to other purposes as well (ex. filtering for observations where a specific user made an identification of a specific taxon).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nfrom pyodide.http import pyfetch # used for asynchronous fetching\nimport asyncio # used for asynchronous fetching\nfrom copy import deepcopy # used for deep copying\nimport math # used for a ceiling method\n#from datetime import datetime # used to convert string datetimes into actual datetimes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'per_page=200&disagreement=true&place_id=110679' # remember: these are filter parameters for identifications, not observations.\nreq_params = parse_qs(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_ids = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/identifications',\n    'max_records': 10000,\n    'max_per_page': 200,\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# basic function to fetch from API and convert repsonse to JSON\nasync def fetchdata(url, method='GET', use_authorization=False, delay=0):\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = deepcopy(req_headers_base)\n        req_headers['Authorization'] = jwt\n    #print(req_headers)\n    #print(f'begin fetch: {method} {url}')\n    response = await pyfetch(url, method=method, headers=req_headers)\n    data = await response.json()\n    print(f'fetch complete: {method} {url}')\n    return data\n\n# function to GET total_results (count) from the API\nasync def gettotalresults(endpoint, params={}, use_authorization=False, delay=0):\n    rp = deepcopy(params)\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    results = await fetchdata(urlwithparams(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(results['total_results'])\n    print(f'total records: {str(total_results)}')\n    return total_results\n              \n# function to GET results from the API\n# if get_all_pages=True, then get all records, up to the limit that the API endpoint provides.\n# query pages in parallel, with each page having a incrementally delayed start.\n# (iNaturalist wants you to limit requests to ~1 req/second.)\nasync def getresults(endpoint, params={}, get_all_pages=False, use_authorization=False):\n    results = []\n    max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n    if get_all_pages:\n        # when getting all pages, make a small query first to find how many total records there are.\n        # this allows us to calculate how many requests we need to make in total.\n        # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n        total_results = await gettotalresults(endpoint, params, use_authorization)\n        total_pages = math.ceil(total_results / endpoint['max_per_page'])\n        if total_pages < max_page:\n            max_page = total_pages\n        print(f'pages to retrieve: {str(max_page)}')\n    async with asyncio.TaskGroup() as tg:\n        tasks = []\n        for i in range(max_page):\n            rp = deepcopy(params)\n            if get_all_pages:\n                # if getting all pages, remove per_page and page parameters if they exist in the base params\n                # and then set per_page = max and increment page for each request\n                rp.pop('per_page', None)\n                rp.pop('page', None)\n                rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                rp['page'] = [str(i+1)]\n            tasks.append(tg.create_task(fetchdata(urlwithparams(endpoint['url'], rp), use_authorization=use_authorization, delay=i)))\n    for t in tasks:\n        data = t.result()\n        #print(data)\n        results+=data['results']\n    print(f'total records retrieved: {str(len(results))}')\n    return results\n\n# function to combine the base url with a set of parameters\n# there's a urlencode method in urllib.parse, but it's easier to get exactly what I need using this custom code.\ndef urlwithparams(url_base, params={}):\n    #print(params)\n    url = url_base\n    for p in list(params.keys()):\n        #print(p)\n        s = '?' if url.find('?') < 0 else '&'\n        pv = ','.join(params[p])\n        url += f'{s}{p}={pv}'\n    # print(url)\n    return url\n\n# function to string together a list observation ids into sets of up to a max number of observations per set\n# the intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for specific observations\ndef obsidstosets(obs_ids, max_set_size=500, separator=',', prefix=''):\n    last_set = math.ceil(len(obs_ids) / max_set_size)\n    set = 0\n    obs_id_sets = []\n    while set < last_set:\n        set += 1\n        range_from = (set - 1) * max_set_size\n        range_to = set * max_set_size\n        range_to = range_to if range_to < len(obs_ids) else len(obs_ids)\n        obs_id_string = prefix + separator.join(map(str, obs_ids[range_from:range_to]))\n        obs_id_sets.append(obs_id_string)\n        print(f'Set {set}: {obs_id_string}')\n    return obs_id_sets",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 1\n\n# get identifications, filtered by the parameters defined in the request parameters (req_params)\nids = await getresults(endpoint_get_ids, req_params, get_all_pages=False, use_authorization=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section -- part 2\n\n# extract the observation ids associated with the identifications\nobs_ids = [oi['observation']['id'] for oi in ids]\n\n# string together the observation IDs, along with with a prefix, to create links to iNaturalist\n# these will be printed in the cell output below. click on the URLs in the output to open a browser tab/window to that URL.\nobs_id_sets = obsidstosets(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# optional execution section\n# this can be used to fetch and accumulate additional identifications after part 1 has already run, without having to change the main request parameters\n# to use the code below, set get_more_ids = True before running.\n\n# if you order by id when you get identifications (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\nget_more_ids = False\nif get_more_ids and ids:\n    rp = deepcopy(req_params)\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([i.get('id') for i in ids])\n            print(f'getting additional identifications for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the identifications we already have\n        else:\n            min_id = min([i.get('id') for i in ids])\n            print(f'getting additional identifcations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the identifications we already have\n        ids += await getresults(endpoint_get_ids, rp, get_all_pages=False, use_authorization=False)\n        print(f'identifications accumulated: {len(ids)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}