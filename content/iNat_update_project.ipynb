{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API Example: Update Project Definition\n- Link: https://jumear.github.io/stirpy/lab?path=iNat_update_project.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "This example shows how to update a project via an undocumented API endpoint `PUT /v1/projects`.\n\nThe primary use case for this is add large sets of project rules to a collection project. For example, if you have a lot of taxa that you want to include in your project, you can define a source set of taxa as (1) your own custom list of ids, (2) extracted from another project, or (3) extracted from an iNaturalist taxon list.\n\nThere is also some bonus code near the end that shows how to update other parts of a project, and you can use / adapt parts of the code here to get lists of taxa from a project or an iNat list.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Get / Update Data from the iNaturalist API",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\nfrom functools import partial # used for pre-loading functions with some arguments\nfrom datetime import datetime # used to convert string datetimes into actual datetimes\nimport json # used to create the body for PUT and POST requests that require a payload\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define custom functions used for getting data\n\ndef params_to_dict(params_string):\n    \"\"\"Convert a parameter string into a dict.\n    ex.: 'taxon_id=1&user_id=kueda,loarie&field:Eating' => {'taxon_id': ['1'], 'user_id': ['kueda', 'loarie'], 'field:Eating': None}\n    \"\"\"\n    params_dict = {pkv[0]: pkv[1].split(',') if len(pkv) > 1 else None for pkv in [ps.split('=') for ps in params_string.split('&')]}\n    return params_dict\n\ndef url_with_params(url_base, params=None):\n    \"\"\"Combine a base url with a set of parameters. Can handle the following types of cases:\n    1. 'https://api.inaturalist.org/v1/observations' + {'taxon_id': [1], 'user_id': ['kueda','loarie']} => 'https://api.inaturlaist.org/v1/observations?taxon_id=1&user_id=kueda,loarie'\n    2. 'https://api.inaturalist.org/v1/places/{id}' + {'id': [1,2,3], 'admin_level': [0,10]} => 'https://www.api.inaturalist.org/v1/places/1,2,3?admin_level=0,10'\n    \"\"\"\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        pv = ','.join(v) if v is not None else None\n        if url.find(pp:=f'{{{p}}}') >= 0:\n            url = url.replace(pp, pv)\n        else:\n            s = '?' if url.find('?') < 0 else '&'\n            url += f'{s}{p}={pv}' if pv is not None else f'{s}{p}'\n    return url\n\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0, body=None):\n    \"\"\"Fetch and convert repsonse to JSON\"\"\"\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = req_headers_base.copy() # make a copy\n        req_headers['Authorization'] = jwt\n    if 'pyfetch' in globals():\n        response = await pyfetch(url, method=method, headers=req_headers, body=body)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers, body=body)\n        data = response.json()\n    print(f'Fetch complete: {method} {url}')\n    return data\n\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    \"\"\"GET total_results (count) from the API\"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data[endpoint.get('record_count_field','total_results')])\n    print(f'Total records: {str(total_results)}')\n    return total_results\n\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    \"\"\"GET a single page of results from the API. Can be called directly but generally is intended to be called by get_results.\n    Additional parsing and additional filtering before and after parsing can happen here, too.\n    \"\"\"\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get(endpoint.get('record_array','results'),[])\n    if pre_parse_filter_function:\n        results = list(filter(pre_parse_filter_function, results))\n    if parse_function:\n        results = parse_function(results)\n    if post_parse_filter_function:\n        results = list(filter(post_parse_filter_function, results))\n    return results\n\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    \"\"\"GET results from the API. When get_all_pages=True, get results over multiple pages using 1 of 2 methods:\n    1. When the endpoint definition includes a page_key field, group key items into batches of up to a max number of records per page / batch.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/taxa/{id}', 'page_key': 'id', 'max_per_page': 30 } and params = {'id': ['1','2','3',...,'60']}\n       Then: GET https://api.inaturalist.org/v1/taxa/1,2,3,...,30; GET https://api.inaturalist.org/v1/taxa/31,32,33,...,60\n    2. In other cases, get pages with the max records per page, up to the maximum record limit that the API endpoint provides.\n       Suppose: endpoint = {'url': 'https://api.inaturalist.org/v1/observations', 'max_records': 10000, 'max_per_page': 200 } and params = {'taxon_id': ['1']}\n       Then: GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=1; GET https://api.inaturalist.org/v1/observations?taxon_id=1&per_page=200&page=2; etc...\n    Get pages in parallel, with each page request having an incrementally delayed start. (iNaturalist suggests limiting requests to ~1 req/second.)\n    \"\"\"\n    if params is None:\n        params = {}\n    results = []\n    if (page_key := endpoint.get('page_key')):\n        if not (page_key_values := params.get(page_key)):\n            print(f'Cannot query from this endpoint without values for {page_key} parameter')\n            return None\n        # if more values are input than the max per page, split these into multiple batches\n        max_per_page = endpoint['max_per_page']\n        total_key_values = len(page_key_values)\n        batches = [page_key_values[i:i+max_per_page] for i in range(0, total_key_values, max_per_page)]\n        print(f'There are {total_key_values} {page_key} values, requiring {len(batches)} API requests to retrieve. Retrieving {\"all sets\" if get_all_pages else \"only the first set\"}...')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in (range(len(batches) if get_all_pages else 1)):\n                rp = params.copy() # make a copy\n                rp[page_key] = batches[i]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    else:\n        max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n        if get_all_pages:\n            # when getting all pages, make a small query first to find how many total records there are.\n            # this allows us to calculate how many requests we need to make in total.\n            # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n            total_results = await get_total_results(endpoint, params, use_authorization)\n            total_pages = math.ceil(total_results / endpoint['max_per_page'])\n            if total_pages < max_page:\n                max_page = total_pages\n            print(f'Pages to retrieve: {str(max_page)}')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in range(max_page):\n                rp = params.copy() # make a copy\n                if get_all_pages:\n                    # if getting all pages, remove per_page and page parameters if they exist in the base params\n                    # and then set per_page = max and increment page for each request\n                    rp.pop('per_page', None)\n                    rp.pop('page', None)\n                    rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                    rp['page'] = [str(i+1)]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    print(f'Total records retrieved: {str(len(results))}')\n    return results\n\ndef parse_simple(results, field_list):\n    \"\"\"Return only specific fields from the results. Only top-level items may be specified in the field list, but children of selected items will be included with the returned values.\"\"\"\n    return [{k: r.get(k) for k in field_list} for r in results]\n\nasync def get_taxa_from_list(list_id, additional_params_string=None, get_all_pages=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    \"\"\"Get a list of taxa from an iNaturalist list, given a list ID and optional additional parameters (ex.\"rank=species\").\n    Note: For some reason, calls to this API endpoint will require authorization. So make sure JWT is defined in global variables.\n    \"\"\"\n    req_params = params_to_dict('order_by=taxon_id&id=' + str(list_id) + (f'&{s}' if (s := additional_params_string) else ''))\n    results = await get_results(endpoint_get_lists, params=req_params, get_all_pages=get_all_pages, use_authorization=True, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function)\n    return results\n    \nasync def get_taxon_ids_from_list(list_id, additional_params_string=None):\n    \"\"\"Get a unique list of taxon_ids from an iNaturalist list, given a list ID and optional additional parameters (ex.\"rank=species\").\n    Note: For some reason, calls to this API endpoint will require authorization. So make sure JWT is defined in global variables.\n    \"\"\"\n    print(f'Getting taxa from list {list_id}...')\n    results = await get_taxa_from_list(list_id, additional_params_string, get_all_pages=True, parse_function=partial(parse_simple,field_list=['taxon_id']))\n    return list(set([r['taxon_id'] for r in results]))\n\nasync def update_project(project_id, body=None):\n    \"\"\"Update a project given a project_id and payload.\n    Note: This requires authorization. So make sure JWT is defined in global variables.\n    \"\"\"\n    results = await fetch_data(url_with_params(endpoint_put_projects_id['url'], params_to_dict(f'id={project_id}')), method='PUT', use_authorization=True, body=body)\n    return results\n\nasync def update_project_rules(project_id, project_rules_updates):\n    \"\"\"Updates a project, given a project id and a list of rules updates\"\"\"\n    max_batch_size = 500 # API requests seem to be blocked if too many deletes are included in the request. so we will process updates in batches.\n    rules_updates_count = len(project_rules_updates)\n    print(f'Processing project rule updates in {str(math.ceil(rules_updates_count/max_batch_size))} batches of up to {str(max_batch_size)} updates...')\n    for i in range(0, rules_updates_count, max_batch_size):\n        print(f'Executing batch {str(int(i/max_batch_size+1))}...')\n        project_update = {\n            'project': {\n                'project_observation_rules_attributes': project_rules_updates[i:i+max_batch_size]\n            }\n        }\n        results = await update_project(project_id, body=json.dumps(project_update))\n\nasync def get_project_rules(project_id, operand_type=None, operator=None, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    req_params = params_to_dict(f'rule_details=true&id={project_id}')\n    project_details = await get_results(endpoint_get_projects_id, req_params, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function)\n    project_rules = [r for r in project_details[0]['project_observation_rules'] if ((operand_type is None or r['operand_type'] == operand_type) and (operand_type is None or r['operator'] == operator))]\n    return project_rules\n\nasync def get_project_rules_operand_ids(project_id, operand_type, operator, pre_parse_filter_function=None, post_parse_filter_function=None):\n    print(f'Getting project rules from project {project_id} for operand_type={operand_type} and operator={operator}...')\n    results = await get_project_rules(project_id, operand_type, operator)\n    return list(set([r['operand_id'] for r in results]))\n\ndef items_to_batches(items, max_batch_size=500, separator=',', prefix=''):\n    \"\"\"String together a list of items into batches of up to a max number of items per set.\n    (The original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for batches of specific observations.)\n    \"\"\"\n    batches = []\n    for i in range(0, len(items), max_batch_size):\n        items_string = prefix + separator.join(map(str, items[i:i+max_batch_size]))\n        batches.append(items_string)\n        print(f'Batch {int(i/max_batch_size+1)}: {items_string}')\n    return batches",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "raw_mimetype": "",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nproject_id = 'pisum-s-personal-project'\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_lists = {\n    'method': 'GET',\n    'url': 'https://www.inaturalist.org/lists/{id}.json',\n    'max_records': 10000,\n    'max_per_page': 200,\n    'record_array': 'listed_taxa',\n    'record_count_field': 'total_entries',\n}\nendpoint_get_projects_id = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/projects/{id}',\n    'max_records': 10000,\n    'max_per_page': 200,\n}\nendpoint_put_projects_id = {\n    'method': 'PUT',\n    'url': 'https://api.inaturalist.org/v1/projects/{id}',\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the model rules that you want to replicate into your project\n# set source_rules[type][include/exclude][ids] = None (or just don't define ids) if you want to skip updates related to these rules\n# set source_rules[type][include/exclude][ids] = [] if you want to delete all existing related rules\n# use get_project_rules_operand_ids(project_id, operand_type, operator) to get a list of operator ids from another project's rules\n# use get_taxon_ids(list_id, additional_params_string) to get a list of taxon ids from an iNatualist list\n\nsource_rules = {\n    'taxon': {\n        'operand_type': 'Taxon',\n        'include': {'operator': 'in_taxon?'},\n        'exclude': {'operator': 'not_in_taxon?'},\n    },\n    'place': {\n        'operand_type': 'Place',\n        'include': {'operator': 'observed_in_place?'},\n        'exclude': {'operator': 'not_observed_in_place?'},\n    },\n    'user': {\n        'operand_type': 'User',\n        'include': {'operator': 'observed_by_user?'},\n        'exclude': {'operator': 'not_observed_by_user?'},\n    },\n}\n\nsource_rules['taxon']['include']['ids'] = await get_taxon_ids_from_list(946645, additional_params_string='rank=species')\n#source_rules['taxon']['include']['ids'] = await get_project_rules_operand_ids('dangerous-plants-animals-and-fungi-of-the-united-kingdom', operand_type=source_rules['taxon']['operand_type'], operator=source_rules['taxon']['include']['operator'])\n#source_rules['taxon']['include']['ids'] = []\n#source_rules['taxon']['include']['ids'] = None\n\nsource_rules['taxon']['exclude']['ids'] = await get_taxon_ids_from_list(4347551)\n#source_rules['taxon']['exclude']['ids'] = []\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Compare the source rules vs the existing target project rules to determine the changes to make on the target project rules\n\nproject_rules = await get_project_rules(project_id)\n#print(project_rules)\n\nproject_rules_updates = [];\nfor source_type_key, source_type in source_rules.items():\n    for source_set_key, source_set in source_type.items():\n        if isinstance(source_set, dict) and (operator := source_set.get('operator')) and (ids := source_set.get('ids') is not None) and (operand_type := source_type.get('operand_type')):\n            existing_rules = [r for r in project_rules if (r['operator'] == operator and r['operand_type'] == operand_type)]\n            print(f'For {source_set_key} {source_type_key} rules:')\n            # ignore existing rules which are also in the source\n            rules_to_ignore = [{'id': r['id'], 'operator': r['operator'], 'operand_type': r['operand_type'], 'operand_id': r['operand_id'], '_destroy': False} for r in existing_rules if r['operand_id'] in source_set['ids']]\n            print(f'- ignore {str(len(rules_to_ignore))} existing rules')\n            # delete existing rules which are not in the source\n            rules_to_delete = [{'id': r['id'], 'operator': r['operator'], 'operand_type': r['operand_type'], 'operand_id': r['operand_id'], '_destroy': True} for r in existing_rules if r['operand_id'] not in source_set['ids']]\n            print(f'- delete {str(len(rules_to_delete))} existing rules')\n            # add new rules for taxa from the source which do not already exist\n            rules_to_add = [{'operator': operator, 'operand_type': operand_type, 'operand_id': s} for s in source_set['ids'] if s not in [r['operand_id'] for r in existing_rules]]\n            print(f'- add {str(len(rules_to_add))} new rules')\n            project_rules_updates += rules_to_delete + rules_to_add",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Execute the request(s) the will finalize the changes\nresults = await update_project_rules(project_id, project_rules_updates)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# get a new snapshot of the project rules\nproject_rules_after_update = await get_project_rules(project_id)\n\nproject_rules_flattened = [\n    {\n        'id': r['id'],\n        'operator': r['operator'],\n        'operand_type': r['operand_type'],\n        'operand_id': r['operand_id'],\n        'id': r['id'],\n        'taxon_name': d['name'] if (d := r.get('taxon')) else None,\n        'taxon_rank': d['rank'] if (d := r.get('taxon')) else None,\n        'place_slug': d['slug'] if (d := r.get('place')) else None,\n        'place_name': d['name'] if (d := r.get('place')) else None,\n        'user_login': d['login'] if (d := r.get('user')) else None,\n    }\n    for r in project_rules_after_update\n]\n#print(project_rules_flattened)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# this is just some bonus code to show how to get taxa from a list\n\n# list_taxa = await get_taxa_from_list(946645, additional_params_string='rank=species', get_all_pages=False)\n# list_taxa_flattened = [{'id': t['id'], 'taxon_id': t['taxon_id'], 'name': t['taxon']['name'], 'rank': t['taxon']['rank']} for t in list_taxa]\n\n#print(list_taxa_flattened)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# this is just some bonus code that shows how to update other parts of the project definition (in this case, the project description)\n\n# project_definition_updates = {\n#     'project': {\n#         'description': 'test'\n#     }\n# }\n# results = await update_project(project_id, body=json.dumps(project_definition_updates))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Write Data to CSV",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ths takes the results retrieved above and writes them to a CSV file. The file will appear in the main folder of the file tree (the topmost tab in the left pane of the JupyterLab interface). Files generated in JupyterLite are saved to the browser's storage. So those will need to be downloaded to a more permanent location if they need to be archived more permanently.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport csv # used to output CSV files",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def data_to_csv(data, csv_filename='export.csv'):\n    \"\"\"Write data to a CSV file\"\"\"\n    csv_fields = list(data[0]) # get fields from the keys of the first record in the dataset\n    with open(csv_filename, 'w', newline='') as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n        csv_writer.writeheader()\n        csv_writer.writerows(data)\n        print(f'Created CSV file {csv_filename} with {len(data)} records.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# export to CSV\ndata_to_csv(project_rules_flattened,'project_rules.csv')\n#data_to_csv(list_taxa_flattened,'list_taxa.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}