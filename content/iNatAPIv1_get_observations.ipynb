{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API v1 Get Observations Example\n- Link: https://jumear.github.io/stirpy/lab?path=iNatAPIv1_get_observations.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Get Data from the iNaturalist API",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This example gets either a single or mutliple pages of results from the [API](https://api.inaturalist.org/). The requests are made asynchronously (in parallel, with a small incremental delay between the initiation of each page request), allowing large recordsets to be fetched in the shortest amount of time while respectng iNaturalist's [suggested request limit](https://www.inaturalist.org/pages/developers) (about 1 per second). The example also provides a model for parsing the results, including flattening some of the items returned in the results. For example, a single observation can be associated with multiple identifications, and this example code can parse those multiple identifications into a single string so that all the identifications can be written out on a single line with the same observation row. (Flattening helps in certain use cases where the data is expected to be tabular, such as when exporting to a CSV file.) The example also provides a model for client-side filtering (as an alternative when server-side filtering is not possible). There are also other examples of how to get counts of observations or a series of counts (ex. observations by state) ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\n#from datetime import datetime # used to convert string datetimes into actual datetimes\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\n    use_pyfetch=True\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().\n    use_pyfetch=False",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'verifiable=true&spam=false'\nreq_params = parse_qs(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_obs = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/observations',\n    'max_records': 10000,\n    'max_per_page': 200,\n}\nendpoint_get_controlled_terms = {\n    'method': 'GET' ,\n    'url': 'https://api.inaturalist.org/v1/controlled_terms',\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# basic function to fetch from API and convert repsonse to JSON\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0):\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = dict(req_headers_base) # make a copy\n        req_headers['Authorization'] = jwt\n    if use_pyfetch:\n        response = await pyfetch(url, method=method, headers=req_headers)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers)\n        data = response.json()\n    print(f'fetch complete: {method} {url}')\n    return data\n\n# function to GET total_results (count) from the API\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    if params is None:\n        params = {}\n    rp = dict(params) # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data['total_results'])\n    print(f'total records: {str(total_results)}')\n    return total_results\n\n# function to GET a single page of results from the API\n# additional parsing and additional filtering before and after the parsing can happen here, too\n# can be called directly but generally is intended to be called through get_results\n# note that the parse_function is expected to be an async function (in case we need to get additional information from the API during parsing)\n# but if the parse_function gets additional data from the API for every page, then the delay used by get_results may need to be tweaked to keep within request limits\n# (ideally, cases where additional data wlll be needed from the API for every page can be handled after all pages have been retreieved)\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    if params is None:\n        params = {}\n    rp = dict(params) # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get('results',[])\n    if pre_parse_filter_function:\n        results = filter(pre_parse_filter_function, results)\n    if parse_function:\n        results = await parse_function(results)\n    if post_parse_filter_function:\n        results = filter(post_parse_filter_function, results)\n    return results\n\n# function to GET results from the API\n# if get_all_pages=True, then get all records, up to the limit that the API endpoint provides.\n# query pages in parallel, with each page having a incrementally delayed start.\n# (iNaturalist wants you to limit requests to ~1 req/second.)\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    if params is None:\n        params = {}\n    results = []\n    max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n    if get_all_pages:\n        # when getting all pages, make a small query first to find how many total records there are.\n        # this allows us to calculate how many requests we need to make in total.\n        # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n        total_results = await get_total_results(endpoint, params, use_authorization)\n        total_pages = math.ceil(total_results / endpoint['max_per_page'])\n        if total_pages < max_page:\n            max_page = total_pages\n        print(f'pages to retrieve: {str(max_page)}')\n    async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n        tasks = []\n        for i in range(max_page):\n            rp = dict(params) # make a copy\n            if get_all_pages:\n                # if getting all pages, remove per_page and page parameters if they exist in the base params\n                # and then set per_page = max and increment page for each request\n                rp.pop('per_page', None)\n                rp.pop('page', None)\n                rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                rp['page'] = [str(i+1)]\n            tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n    for t in tasks:\n        results += t.result()\n    print(f'total records retrieved: {str(len(results))}')\n    return results\n\n# function used by another function get_field_value to get a particular value from a results row\ndef get_ref_value(rec, ref):\n    # if the reference is chained (ex. taxon.id), then split these apart, and iterate through each object / dict.\n    # if any of the references is an index for an array / list (ex. index 0 in photos[0].id), then handle those, too.\n    ref_chain = ref.split('.')\n    value = rec\n    for r in ref_chain:\n        items = [];\n        if r.find('[') >= 0:\n            r = r.replace(']','')\n            r = r.split('[')\n            items = r[1:len(r)]\n            r = r[0]\n        value = value.get(r)\n        if value is None:\n            break\n        if items:\n            for i in map(int, items):\n                if len(value or []) == 0:\n                    value = None\n                    break\n                value = value[i]\n        if value is None:\n            break\n    return value\n\n# function used by another function get_field_value to filter a nested list value based on certain filter parameters\ndef filter_ref_value(rec, value, params):\n    filtered = []\n    for r in value:\n        for f in params:\n            if not (get_ref_value(r,f.get('ref')) == (get_ref_value(rec, fvr) if (fvr := f.get('value_ref')) else f.get('value'))):\n                break\n        else:\n            filtered.append(r)\n    return filtered\n\n# function used by another function parse_results to parse a results row and get / calculate the value for a particular field\ndef get_field_value(rec, field):\n    # core processing\n    value = get_ref_value(rec, field['ref']) if field['ref'] else rec \n    if value is None and field.get('alt'):\n        value = get_ref_value(rec, field['alt'])\n    if (ff := field.get('function')) == 'count':\n        value = len(value or [])\n    elif value is not None:\n        fp = field.get('params',{})\n        if ff == 'split':\n            value = value.split(fp.get('separator'))[fp.get('index')]\n            try:\n                value = int(value)\n            except:\n                try:\n                    value = float(value)\n                except:\n                    pass\n        elif ff == 'join':\n            value = value = fp.get('separator').join(map(str, value)) if value else None\n        elif ff == 'replace':\n            value = value.replace(fp.get('old_text'), fp.get('new_text'))\n        elif ff == 'combine':\n            cvalue = fp.get('template','')\n            cref = fp.get('combine_refs',[])\n            for i, cr in enumerate(cref):\n                cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(value,cr)))\n            value = cvalue\n        elif ff == 'filter_combine':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = []\n            for r in filtered:\n                cvalue = fp.get('template','')\n                cref = fp.get('combine_refs',[])\n                for i, cr in enumerate(cref):\n                    cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(r,cr)))\n                fvalue.append(cvalue)\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n            if value == []:\n                value = None\n        elif ff == 'filter_count':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            value = len(filtered) if (dr := fp.get('distinct_ref')) is None else len(set([get_ref_value(r, dr) for r in filtered])) # get a distinct count if distinct_ref is defined\n        elif ff == 'filter_select':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = [get_ref_value(r, fp.get('select_ref')) for r in filtered]\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n    return value\n\n# function passed by get_obs to get_results to parse a set of observation results\nasync def parse_obs(results):\n    # each dict in the field definition must have at least a ref (reference) key. (note: if ref is set to None, the observation row will be retrieved as the value.)\n    # use an optional label if you want the key to be different from the ref.\n    # use an optional alt (alternative reference) if you want a fallback in case no data is found in ref.\n    # use optional function + params to do more complicated parsing of the ref.\n    # really complicated logic can be handled with a little additional processing using parse_options. \n    parse_fields = [\n        {'ref': 'id'},\n        #{'label': 'url', 'ref': None, 'function': 'combine', 'params': {'combine_refs': ['id'], 'template': 'https://www.inaturalist.org/observations/{0}'}},\n        #{'ref': 'uuid'},\n        {'ref': 'quality_grade'},\n        #{'label': 'user_id', 'ref': 'user.id'},\n        {'label': 'user_login', 'ref': 'user.login'},\n        {'label': 'user_login_id', 'ref': 'user', 'function': 'combine', 'params': {'combine_refs': ['login','id'], 'template': '{0} ({1})'}},\n        #{'label': 'user_name', 'ref': 'user.name'},\n        #{'label': 'taxon_ancestors', 'ref': 'taxon.ancestors', 'function': 'filter_combine', 'params': {'combine_refs': ['name','rank','id'], 'template': '{0} ({1}) ({2})', 'separator': ', '}},\n        #{'label': 'kingdom', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'kingdom'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'phylum', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'phylum'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'class', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'class'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'order', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'order'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'family', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'family'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'genus', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'genus'}], 'select_ref': 'name', 'separator': ', '}},\n        #{'label': 'species', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'species'}], 'select_ref': 'name', 'separator': ', '}},\n        {'label': 'taxon_id', 'ref': 'taxon.id'},\n        {'label': 'taxon_name', 'ref': 'taxon.name'},\n        {'label': 'taxon_preferred_common_name', 'ref': 'taxon.preferred_common_name'},\n        {'label': 'taxon_rank', 'ref': 'taxon.rank'},\n        #{'label': 'taxon_rank_level', 'ref': 'taxon.rank_level'},\n        #{'label': 'taxon_ancestry', 'ref': 'taxon.ancestry'},\n        #{'ref': 'observed_on_string'},\n        {'ref': 'time_observed_at'},\n        {'ref': 'created_at'},\n        #{'ref': 'updated_at'},\n        {'ref': 'place_guess'},\n        #{'ref': 'location'},\n        {'label': 'latitude', 'ref': 'location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n        {'label': 'longitude', 'ref': 'location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n        {'ref': 'public_positional_accuracy'},\n        #{'ref': 'private_place_guess'},\n        #{'ref': 'private_location'},\n        #{'label': 'private_latitude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n        #{'label': 'private_longitiude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n        #{'ref': 'positional_accuracy'},\n        {'ref': 'taxon_geoprivacy'},\n        {'ref': 'privacy'},\n        {'ref': 'description'},\n        {'label': 'photos_count', 'ref':'photos', 'function': 'count'},\n        #{'label': 'photo_1_id', 'ref': 'photos[0].id'},\n        {'label': 'photo_1_url', 'ref': 'photos[0].url', 'function': 'replace', 'params': {'old_text': 'square', 'new_text': 'medium'}}, # size options are thumb, square, small, medium, large, and original\n        {'label': 'photo_1_license_code', 'ref': 'photos[0].license_code'},\n        {'label': 'sounds_count', 'ref':'sounds', 'function': 'count'},\n        {'ref': 'comments_count'},\n        #{'label': 'others_current_identifications_count', 'ref': 'identifications_count'},\n        {'label': 'current_identifications_count', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}]}},\n        #{'label': 'current_identifications_by_observer', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}]}},\n        #{'label': 'current_identification_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}], 'select_ref': 'taxon.name', 'separator': ', '}},\n        #{'label': 'current_identification_category_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}], 'select_ref': 'category', 'separator': ', '}},\n        #{'ref': 'owners_identification_from_vision'},\n        {'label': 'prefers_community_taxon', 'ref': 'preferences.prefers_community_taxon', 'alt': 'user.preferences.prefers_community_taxa'},\n        #{'label': 'identifier_ids', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.id', 'separator': ', '}},\n        #{'label': 'identifier_logins', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.login', 'separator': ', '}},\n        {'label': 'identifications', 'ref': 'identifications', 'function': 'filter_combine', 'params': {'filter': [{'ref': 'current', 'value': True}], 'combine_refs': ['user.login','taxon.name','taxon.id'], 'template': '{0}: {1} ({2})', 'separator': ', '}},\n        #{'label': 'identification_date_first', 'ref': 'identifications[0].created_at'},\n        #{'label': 'identification_date_last', 'ref': 'identifications[-1].created_at'},\n        #{'label': 'identifications_vs_obs', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'vs_obs', 'separator': ', '}},\n        {'label': 'identifications_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'same'}]}},\n        #{'label': 'ident_taxa_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'same'}], 'distinct_ref': 'taxon.id'}},\n        #{'label': 'identifications_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'ancestor'}]}},\n        {'label': 'ident_taxa_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'ancestor'}], 'distinct_ref': 'taxon.id'}},\n        #{'label': 'identifications_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'descendant'}]}},\n        {'label': 'ident_taxa_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'descendant'}], 'distinct_ref': 'taxon.id'}},\n        #{'label': 'identifications_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'different'}]}},\n        {'label': 'ident_taxa_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'vs_obs', 'value': 'different'}], 'distinct_ref': 'taxon.id'}},\n        {'label': 'reviewed_by_count', 'ref': 'reviewed_by', 'function': 'count'},\n        #{'ref': 'reviewed_by', 'function': 'join', 'params': {'separator':', '}},\n        #{'ref': 'captive'},\n        {'label': 'annotations_count','ref':'annotations', 'function': 'count'},\n        #{'label': 'annotations_ids', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute_id','controlled_value_id'], 'template': '{0}:{1}', 'separator': ', '}},\n        {'label': 'annotations', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute','controlled_value'], 'template': '{0}: {1}', 'separator': ', '}}, # note: this relies on some pre-procesing to create the controlled_attribute and controlled_value fields\n        {'label': 'observation_fields_count', 'ref':'ofvs', 'function': 'count'},\n        #{'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','value'], 'template': '{0} ({1}): {2}', 'separator': '; '}},\n        {'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','taxon_or_value'], 'template': '{0} ({1}): {2}', 'separator': '; '}}, # note: this relies on pre-procesing to create a field that contains either taxon or value\n        {'label': 'tags_count', 'ref':'tags', 'function': 'count'},\n        {'ref': 'tags', 'function': 'join', 'params': {'separator': ', '}},\n        #{'ref': 'oauth_application_id'},\n        #{'ref': 'site_id'},\n        {'label': 'gbif_occurence_url', 'ref': 'outlinks', 'function': 'filter_select', 'params': {'filter': [{'ref': 'source', 'value': 'GBIF'}], 'select_ref': 'url', 'separator': ', '}},\n    ]\n    # define some parse options for more complicated stiuations\n    parse_field_refs = [pf.get('ref') for pf in parse_fields]\n    parse_options = [];\n    if 'annotations' in parse_field_refs:\n        await get_annotations()\n        parse_options.append('annotation_descriptions')\n    if 'ofvs' in parse_field_refs:\n        parse_options.append('observation_field_taxon_or_value')\n    if 'identifications' in parse_field_refs:\n        parse_options.append('identifications')\n    if 'taxon.ancestors' in parse_field_refs:\n        parse_options.append('taxon_ancestors')\n    # parse based on the parse_fields defintion\n    presults = []\n    for r in results:\n        # handle some special pre-processing parse_options at the row level\n        if 'annotation_descriptions' in parse_options:\n            for a in r.get('annotations',[]):\n                a['controlled_attribute'] = get_annotations.xref[a['controlled_attribute_id']]\n                a['controlled_value'] = get_annotations.xref[a['controlled_value_id']]\n        if 'observation_field_taxon_or_value' in parse_options:\n            for of in r.get('ofvs',[]):\n                of['taxon_or_value'] = f'{of[\"taxon\"][\"name\"]} ({of[\"taxon\"][\"id\"]})' if of['datatype'] == 'taxon' and of.get('taxon') else of['value']\n        if 'identifications' in parse_options:\n            #ic = 0\n            for i, id in enumerate(r.get('identifications',[])):\n                #id['seq'] = i+1\n                #if id['current'] == 'true':\n                #    ic += 1\n                #    id['seq_current'] = ic\n                if not (rt := r.get('taxon')) or not (idt := id.get('taxon')):\n                    id['vs_obs'] = 'none'\n                elif rt['id'] == idt['id']:\n                    id['vs_obs'] = 'same'\n                elif (idta := idt.get('ancestry')) is not None and rt['id'] in map(int, idta.split('/')):\n                    id['vs_obs'] = 'descendant'\n                elif (rta := rt.get('ancestry')) is not None and idt['id'] in map(int, rta.split('/')):\n                    id['vs_obs'] = 'ancestor'\n                else:\n                    id['vs_obs'] = 'different'\n        if 'taxon_ancestors' in parse_options:\n            ancestors = []\n            rank_level_kingdom = 70 # this is the highest-level taxon stored in identification[i].ancestors\n            if (rt := r.get('taxon')) and (taxon_id := rt.get('id')) is not None and (rank_level := rt.get('rank_level')) < rank_level_kingdom:\n                for id in r.get('identifications',[]):\n                    if (idt := id.get('taxon')):\n                        if idt['id'] == taxon_id:\n                            ancestors = list(idt['ancestors'])\n                            break\n                        if (idta := idt['ancestors']):\n                            for i, atid in enumerate([a['id'] for a in idta]):\n                                if atid == taxon_id:\n                                    ancestors = idta[0:i] # add everything above this taxon (will add this taxon later below)\n                                    break\n                        if ancestors:\n                            break\n            if rt and rank_level <= rank_level_kingdom:\n                ancestors.append(dict(rt))\n                rt['ancestors'] = ancestors\n        # core processing\n        row = {}\n        for i, f in enumerate(parse_fields):\n            row[f.get('label') or f.get('ref') or f'col_{i+1}'] = get_field_value(r,f)\n        presults.append(row)\n    return presults\n\n# function to get and parse observations\nasync def get_obs(params=None, get_all_pages=False, use_authorization=False, parse_function=parse_obs, pre_parse_filter_function=None, post_parse_filter_function=None):\n    if params is None:\n        params = {}\n    if params.get('only_id',['false']) == ['true']: # if only_id=true, then don't parse fields because only id will exist in the results\n        parse_function = None\n    results = await get_results(endpoint_get_obs, params, get_all_pages, use_authorization, parse_function, pre_parse_filter_function, post_parse_filter_function)\n    return results\n \n# function to get a series of counts\n# base_params are the (fixed) parameters that will be applied when getting the count for each item in the series.\n# series_params is a list of (variable) parameters (keys) to add to base_params for each item in the series.\n# series is a list of dicts, each of which defines the parameter key/value pairs for each item in the series.\n# each item in the series list can contain additional attributes that are not parameters, and it does not have to contain all the keys in the series_params list.\n# if add_count_to_series is set to True, the function will add the counts to the original series object; otherwise, it just returns a (deep) copy of series with counts.\nasync def get_count_series(endpoint, series, series_params, base_params=None, count_label='rec_count', use_authorization=False, add_count_to_series=False):\n    if base_params is None:\n        base_params = {}\n    if not series or not series_params:\n        print(f'The series parameter must be a list of dicts with keys that include the values in the list passed in for series_params.')\n        return None\n    rv = []\n    results = series if add_count_to_series else list(series) # results will look the same, but if add_count_to_series=True, the original series list wlll actually change\n    async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n        tasks = []\n        for i, r in enumerate(results):\n            rp = dict(base_params)\n            for sp in series_params:\n                if (spv := r.get(sp)) is not None:\n                    rp.pop(sp, None)\n                    rp[sp] = [str(spv)]\n            tasks.append(tg.create_task(get_total_results(endpoint, rp, use_authorization=use_authorization, delay=i)))\n    for i, t in enumerate(tasks):\n        results[i][count_label] = t.result()\n    return results\n\n# function to combine the base url with a set of parameters\n# there's a urlencode method in urllib.parse, but it's easier to get exactly what I need using this custom code.\ndef url_with_params(url_base, params=None):\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        s = '?' if url.find('?') < 0 else '&'\n        pv = ','.join(v)\n        url += f'{s}{p}={pv}'\n    return url\n\n# function to get annotation ids and descriptions from the API\n# only the ids are included in the GET /v1/observations response. so a cross-reference is needed to translate the ids to plain English.\n# reults are stored in an attribute on the function called xref so that it won't be necessary to get data from the APi more than once\nasync def get_annotations():\n    xref = getattr(get_annotations, 'xref', None)\n    if xref is None:\n        xref = {};\n        terms = await(fetch_data(endpoint_get_controlled_terms['url']))\n        for t in terms['results']:\n            xref[t['id']] = t['label']\n            for v in t['values']:\n               xref[v['id']] = v['label']\n        print(f'retrieved annnotation cross-references ({len(xref)} items)')\n        get_annotations.xref = xref\n    return xref\n\n# function to string together a list of observation ids into sets of up to a max number of observations per set\n# the original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for specific observations\ndef obs_ids_to_sets(obs_ids, max_set_size=500, separator=',', prefix=''):\n    obs_id_sets = []\n    for i in range(0, len(obs_ids), max_set_size):\n        obs_id_string = prefix + separator.join(map(str, obs_ids[i:i+max_set_size]))\n        obs_id_sets.append(obs_id_string)\n        print(f'Set {int(i/max_set_size+1)}: {obs_id_string}')\n    return obs_id_sets",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section\n\n# get observations\nobs = await get_obs(req_params, get_all_pages=False, use_authorization=False)\n#obs\n\n# when possible, it's always best to filter on the server side by using filter parameters when making API requests.\n# but when a particular filter is not available in the API, it may still be possible to filter on the client side (as opposed to server side)\n# here's an example of how to do client-side filtering for observations which have >1 (current) identification using post_parse_filter_function\n# use pre_parse_filter_function when you can filter based on the results directly from the API response.\n# use post_parse_filter_function when you must rely on the values in a parsed field to do the filtering.\n# (you can always filter separately *after* getting observations, of course, but filtering *while* getting obs saves on system memory when getting multiple pages of results from the API.)\n#obs = await get_obs(req_params, get_all_pages=False, use_authorization=False, post_parse_filter_function=(lambda x: x['current_identifications_count'] > 1))\n\n# get observation ids from obs\n#obs_ids = [o.get('id') for o in obs]\n#obs_id_sets = obs_ids_to_sets(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')\n\n# get just total results (count)\n#obs_count = await get_total_results(endpoint_get_obs, req_params, use_authorization=False)\n#obs_count\n\n# get a series of counts\n#obs_count_series = [\n#    {'label': 'Texas 2020', 'year': 2020, 'place_id': 18},\n#    {'label': 'not Texas 2020', 'year': 2020, 'not_in_place': 18},\n#    {'label': 'Texas 2021', 'year': 2021, 'place_id': 18},\n#    {'label': 'not Texas 2021', 'year': 2021, 'not_in_place': 18},\n#]\n#await get_count_series(endpoint_get_obs, obs_count_series, ['year','place_id','not_in_place'], base_params=req_params, count_label='obs_count', use_authorization=False, add_count_to_series=True)\n#obs_count_series",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# if you order by id when you get observations (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\n# to use the code below, set get_more_obs = True before running.\nget_more_obs = False\n#if get_more_obs and obs and len(obs) >= endpoint_get_obs['max_records'] and len(obs) % endpoint_get_obs['max_records'] == 0:\nif get_more_obs and obs:\n    rp = dict(req_params) # make a copy\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([o.get('id') for o in obs])\n            print(f'getting additional observations for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the obs we already have\n        else:\n            min_id = min([o.get('id') for o in obs])\n            print(f'getting additional observations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the obs we already have\n        obs += await get_obs(rp, get_all_pages=True, use_authorization=False)\n        print(f'observations accumulated: {len(obs)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Write Data to CSV",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Ths takes the results retrieved above and writes them to a CSV file. The file will appear in the main folder of the file tree (the topmost tab in the left pane of the JupyterLab interface). Files generated in JupyterLite are saved to the browser's storage. So those will need to be downloaded to a more permanent location if they need to be archived more permanently.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport csv # used to output CSV files",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function write data to a CSV file\ndef data_to_csv(data, csv_filename='export.csv'):\n    csv_fields = list(data[0]) # get fields from the keys of the first record in the dataset\n    with open(csv_filename, 'w', newline='') as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n        csv_writer.writeheader()\n        csv_writer.writerows(data)\n        print(f'created CSV file {csv_filename} with {len(data)} records.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# export to CSV\ndata_to_csv(obs,'observations.csv')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Work with Data in a DataFrame",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since many Python analysis / visualization modules and workflows rely on getting data into a `pandas` dataframe, this provides a very barebones example of getting the data into a dataframe. The dataframe should generally handle most of the data type conversions, but there's a little bit more effort to get dates into a datetime typed column in the dataframe.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# load data into a DataFrame (df)\ndf = pd.DataFrame(obs)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Get basic summary statistics for df\ndf.describe()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Preview the contents of the df\ndf",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# convert datetime columns to datetimes, localized to UTC\nfor k in ['time_observed_at','created_at','updated_at']:\n    if k in df.columns:\n        try:\n            df[k] = pd.to_datetime(df[k], utc=True, errors='coerce')\n        except:\n            print(f'could not convert column {k} to datetime')\n\n# get count (of id) by observed year\ndf.groupby(df.time_observed_at.dt.year).id.count()\n\n# get count (of id) by created year\n#df.groupby(df.created_at.dt.year).id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# filtering example: records where observation_fields are not null\ndf.loc[df.observation_fields.notnull()]\n\n# count (of id of) records where acc > 100\n# df.loc[df.public_positional_accuracy > 100].id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}