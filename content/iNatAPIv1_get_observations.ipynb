{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API v1 Get Observations Example\n- Link: https://jumear.github.io/stirpy/lab?path=iNatAPIv1_get_observations.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Get Data from the iNaturalist API",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nfrom pyodide.http import pyfetch # used for asynchronous fetching\nimport asyncio # used for asynchronous fetching\nfrom copy import deepcopy # used for deep copying\nimport math # used for a ceiling method\n#from datetime import datetime # used to convert string datetimes into actual datetimes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'verifiable=true&spam=false&user_id=pisum'\nreq_params = parse_qs(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define the GET /v1/observations endpoint\nendpoint_get_obs = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/observations',\n    'max_records': 10000,\n    'max_per_page': 200,\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# basic function to fetch from API and convert repsonse to JSON\nasync def fetchdata(url, method='GET', use_authorization=False, delay=0):\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = deepcopy(req_headers_base)\n        req_headers['Authorization'] = jwt\n    #print(req_headers)\n    #print(f'begin fetch: {method} {url}')\n    response = await pyfetch(url, method=method, headers=req_headers)\n    data = await response.json()\n    print(f'fetch complete: {method} {url}')\n    return data\n\n# function to GET total_results (count) from the API\nasync def gettotalresults(endpoint, params={}, use_authorization=False, delay=0):\n    rp = deepcopy(params)\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    results = await fetchdata(urlwithparams(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = results['total_results']\n    print (f'total records: {str(total_results)}')\n    return total_results\n\n# function to GET results from the API\n# if get_all_pages=True, then get all records, up to the limit that the API endpoint provides.\n# query pages in parallel, with each page having a incrementally delayed start.\n# (iNaturalist wants you to limit requests to ~1 req/second.)\nasync def getresults(endpoint, params={}, get_all_pages=False, use_authorization=False):\n    results = []\n    max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n    if get_all_pages:\n        # when getting all pages, make a small query first to find how many total records there are.\n        # this allows us to calculate how many requests we need to make in total.\n        # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n        total_results = gettotalresults(endpoint, params, use_authorization)\n        total_pages = math.ceil(total_results / endpoint['max_per_page'])\n        if total_pages < max_page:\n            max_page = total_pages\n        print (f'pages to retrieve: {str(max_page)}')\n    async with asyncio.TaskGroup() as tg:\n        tasks = []\n        for i in range(max_page):\n            rp = deepcopy(params)\n            if get_all_pages:\n                # if getting all pages, remove per_page and page parameters if they exist in the base params\n                # and then set per_page = max and increment page for each request\n                rp.pop('per_page', None)\n                rp.pop('page', None)\n                rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                rp['page'] = [str(i+1)]\n            tasks.append(tg.create_task(fetchdata(urlwithparams(endpoint['url'], rp), use_authorization=use_authorization, delay=i)))\n    for t in tasks:\n        data = t.result()\n        #print(data)\n        results+=data['results']\n    print (f'total records retrieved: {str(len(results))}')\n    return results\n\n# function used by another function getfieldvalue to get a particular value from a results row\ndef getrefvalue(rec, ref):\n    # if the reference is chained (ex. taxon.id), then split these apart, and iterate through each object / dict.\n    # if any of the references is an index for an array / list (ex. index 0 in photos[0].id), then handle those, too.\n    ref_chain = ref.split('.')\n    value = rec\n    for r in ref_chain:\n        items = [];\n        if r.find('[') >= 0:\n            r = r.replace(']','')\n            r = r.split('[')\n            items = r[1:len(r)]\n            r = r[0]\n        #print(r)\n        #print(items)\n        value = value.get(r)\n        if value is None:\n            break\n        if len(items) > 0:\n            for i in map(int, items):\n                if len(value or []) == 0:\n                    value = None\n                    break\n                value = value[i]\n        if value is None:\n            break\n    return value\n\n# function used by another function parseresults to parse a results row and get / calculate the value for a particular field\ndef getfieldvalue(rec, field):\n    value = getrefvalue(rec, field['ref'])\n    if value is None and field.get('alt') is not None:\n        value = getrefvalue(rec, field['alt'])\n    if field.get('function') == 'count':\n        value = len(value or [])\n    elif value is not None:\n        fp = field.get('params',{})\n        if field.get('function') == 'split':\n            value = value.split(fp.get('separator'))[fp.get('index')]\n            try:\n                value = int(value)\n            except:\n                try:\n                    value = float(value)\n                except:\n                    None\n        elif field.get('function') == 'join':\n            value = value = fp.get('separator').join(map(str, value)) if len(value) > 0 else None\n        elif field.get('function') == 'replace':\n            value = value.replace(fp.get('old_text'), fp.get('new_text'))\n        elif field.get('function') == 'multiselect':\n            fvalue = []\n            for r in value:\n                sv = []\n                for sr in fp.get('select_refs'):\n                    sv.append(str(getrefvalue(r,sr)))\n                svalue = fp.get('template')\n                for i in range(len(sv)):\n                    svalue = svalue.replace(f'{{{i}}}',sv[i])\n                fvalue.append(svalue)\n            value = fp.get('separator').join(map(str, fvalue)) if len(fvalue) > 0 else None\n        elif field.get('function') == 'filtercount':\n            count = 0\n            for r in value:\n                match = True\n                for f in fp.get('filter'):\n                    if f.get('value'):\n                        match = match and getrefvalue(r,f.get('ref')) == f['value']\n                    elif f.get('value_ref'):\n                        match = match and getrefvalue(r,f.get('ref')) == getrefvalue(rec, f['value_ref'])\n                    else:\n                        match = False\n                if match:\n                    count += 1\n            value = count\n        elif field.get('function') == 'filterselect':\n            fvalue = []\n            for r in value:\n                match = True\n                for f in fp.get('filter'):\n                    if f.get('value'):\n                        match = match and getrefvalue(r,f.get('ref')) == f['value']\n                    elif f.get('value_ref'):\n                        match = match and getrefvalue(r,f.get('ref')) == getrefvalue(rec, f['value_ref'])\n                    else:\n                        match = False\n                if match:\n                    fvalue.append(getrefvalue(r, fp.get('select_ref')))\n            value = fp.get('separator').join(map(str, fvalue)) if len(fvalue) > 0 else None\n    return value\n\n# funtion used to parse a results set based on a set of field definitions \ndef parseresults(results, fields=[]):\n    presults = []\n    for r in results:\n        #print (r['id'])\n        row = {}\n        for f in fields:\n            row[f.get('label') or f.get('ref')] = getfieldvalue(r,f)\n        presults.append(row)\n    return presults\n\n# function to get and parse observations\n# get the results from API using function getresults.\n# define the data we want in parse_fields.\n# use function parseresults to transform the results to the data we want.\nasync def getobs(params={}, get_all_pages=False, use_authorization=False):\n    results = await getresults(endpoint_get_obs, params, get_all_pages, use_authorization)\n    # each object in the field definition must have at least a ref (reference).\n    # use an optional label if you want the key to be different from the ref.\n    # use an optional alt (alternative reference) if you want a fallback in case no data is found in ref.\n    # use optional function + params to do more complicated parsing of the ref.\n    parse_fields = [\n        {'ref': 'id'},\n        #{'ref': 'uuid'},\n        {'ref': 'quality_grade'},\n        #{'label': 'user_id', 'ref': 'user.id'},\n        {'label': 'user_login', 'ref': 'user.login'},\n        #{'label': 'user_name', 'ref': 'user.name'},\n        {'label': 'taxon_id', 'ref': 'taxon.id'},\n        {'label': 'taxon_name', 'ref': 'taxon.name'},\n        {'label': 'taxon_preferred_common_name', 'ref': 'taxon.preferred_common_name'},\n        {'label': 'taxon_rank', 'ref': 'taxon.rank'},\n        #{'label': 'taxon_rank_level', 'ref': 'taxon.rank_level'},\n        #{'label': 'taxon_ancestry', 'ref': 'taxon.ancestry'},\n        #{'ref': 'observed_on_string'},\n        {'ref': 'time_observed_at'},\n        {'ref': 'created_at'},\n        #{'ref': 'updated_at'},\n        {'ref': 'place_guess'},\n        #{'ref': 'location'},\n        {'label': 'latitude', 'ref': 'location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n        {'label': 'longitude', 'ref': 'location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n        {'ref': 'public_positional_accuracy'},\n        #{'ref': 'private_place_guess'},\n        #{'ref': 'private_location'},\n        #{'label': 'private_latitude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n        #{'label': 'private_longitiude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n        #{'ref': 'positional_accuracy'},\n        {'ref': 'taxon_geoprivacy'},\n        {'ref': 'privacy'},\n        {'ref': 'description'},\n        {'label': 'photos_count', 'ref':'photos', 'function': 'count'},\n        #{'label': 'photo_1_id', 'ref': 'photos[0].id'},\n        {'label': 'photo_1_url', 'ref': 'photos[0].url', 'function': 'replace', 'params': {'old_text': 'square', 'new_text': 'medium'}}, # size options are thumb, square, small, medium, large, and original\n        {'label': 'photo_1_license_code', 'ref': 'photos[0].license_code'},\n        {'label': 'sounds_count', 'ref':'sounds', 'function': 'count'},\n        {'ref': 'comments_count'},\n        #{'label': 'others_current_identifications_count', 'ref': 'identifications_count'},\n        {'label': 'current_identifications_count', 'ref': 'identifications', 'function': 'filtercount', 'params': {'filter': [{'ref': 'current', 'value': True}]}},\n        #{'label': 'current_identifications_by_observer', 'ref': 'identifications', 'function': 'filtercount', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}]}},\n        {'label': 'current_identification_by_observer', 'ref': 'identifications', 'function': 'filterselect', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}], 'select_ref': 'taxon.name', 'separator': ', '}},\n        {'label': 'current_identification_category_by_observer', 'ref': 'identifications', 'function': 'filterselect', 'params': {'filter': [{'ref': 'current', 'value': True}, {'ref': 'user.id', 'value_ref': 'user.id'}], 'select_ref': 'category', 'separator': ', '}},\n        {'ref': 'owners_identification_from_vision'},\n        {'label': 'prefers_community_taxon', 'ref': 'preferences.prefers_community_taxon', 'alt': 'user.preferences.prefers_community_taxa'},\n        #{'label': 'identifier_ids', 'ref': 'identifications', 'function': 'filterselect', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.id', 'separator': ', '}},\n        {'label': 'identifier_logins', 'ref': 'identifications', 'function': 'filterselect', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.login', 'separator': ', '}},\n        {'label': 'reviewed_by_count', 'ref': 'reviewed_by', 'function': 'count'},\n        #{'ref': 'reviewed_by', 'function': 'join', 'params': {'separator':', '}},\n        #{'ref': 'captive'},\n        {'label': 'annotations_count','ref':'annotations', 'function': 'count'},\n        {'label': 'annotations', 'ref': 'annotations', 'function': 'multiselect', 'params': {'select_refs': ['controlled_attribute_id','controlled_value_id'], 'template': '{0}:{1}', 'separator': ', '}},\n        {'label': 'observation_fields_count', 'ref':'ofvs', 'function': 'count'},\n        {'label': 'observation_fields', 'ref': 'ofvs', 'function': 'multiselect', 'params': {'select_refs': ['name','field_id','value'], 'template': '{0} ({1}): {2}', 'separator': '; '}},\n        {'label': 'tags_count', 'ref':'tags', 'function': 'count'},\n        {'ref': 'tags', 'function': 'join', 'params': {'separator':', '}},\n        #{'ref': 'oauth_application_id'},\n        #{'ref': 'site_id'},\n        {'label': 'gbif_occurence_url', 'ref': 'outlinks', 'function': 'filterselect', 'params': {'filter': [{'ref': 'source', 'value': 'GBIF'}], 'select_ref': 'url', 'separator': ', '}},\n    ]\n    obs = parseresults(results, parse_fields)\n    print (f'observations parsed: {str(len(obs))}')\n    return obs\n\n# function to get a set of observation ids only\n# if a separator string parameter is passed in, the function will return a string where the ids are separated by that separator.\n# otherwise, the function will return a list of ids\nasync def getobsid(params={}, get_all_pages=False, use_authorization=False, separator=None):\n    rp = deepcopy(params)\n    rp['only_id'] = ['true'] # set this to true, since we only want ids\n    obs = await getresults(endpoint_get_obs, rp, get_all_pages, use_authorization)\n    obs = [o.get('id') for o in obs]\n    print (f'observations parsed: {str(len(obs))}')\n    if separator:\n        obs = separator.join(map(str,obs))\n    return obs\n\n# function to get a series of counts\n# base_params are the (fixed) parameters that will be applied when getting the count for each item in the series.\n# series_params is a list of (variable) parameters (keys) to add to base_params for each item in the series.\n# series is a list of dicts, each of which defines the parameter key/value pairs for each item in the series.\n# each item in the series list can contain additional attributes that are not parameters, and it does not have to contain all the keys in the series_params list.\n# if add_count_to_series is set to True, the function will add the counts to the original series object; otherwise, it just returns a (deep) copy of series with counts.\nasync def getcountseries(endpoint, series, series_params, base_params={}, count_label='rec_count', use_authorization=False, add_count_to_series=False):\n    if len(series) == 0 or len(series_params) == 0:\n        print(f'The series parameter must be a list of dicts with keys that include the values in the list passed in for series_params.')\n        return None\n    rv = []\n    results = series if add_count_to_series else deepcopy(series) # results will look the same, but if add_count_to_series=True, the original series list wlll actually change\n    async with asyncio.TaskGroup() as tg:\n        tasks = []\n        for i in range(len(results)):\n            rp = deepcopy(base_params)\n            for sp in series_params:\n                if results[i].get(sp) is not None:\n                    rp.pop(sp, None)\n                    rp[sp] = [str(results[i].get(sp))] \n            #print(rp)\n            tasks.append(tg.create_task(gettotalresults(endpoint, rp, use_authorization=use_authorization, delay=i)))\n    for t in range(len(tasks)):\n        results[t][count_label] = tasks[t].result()\n    return results\n\n# function to combine the base url with a set of parameters\n# there's a urlencode method in urllib.parse, but it's easier to get exactly what I need using this custom code.\ndef urlwithparams(url_base, params={}):\n    #print(params)\n    url = url_base\n    for p in list(params.keys()):\n        #print(p)\n        s = '?' if url.find('?') < 0 else '&'\n        pv = ','.join(params[p])\n        url += f'{s}{p}={pv}'\n    # print(url)\n    return url",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section\n\n# get observations\nobs = await getobs(req_params, get_all_pages=False, use_authorization=False)\n#obs\n\n# get observation ids from obs\n#obs_ids = ','.join(map(str,[o.get('id') for o in obs]))\n\n# get observation ids without first getting obs\n#obs_ids = await getobsid(req_params, get_all_pages=False, use_authorization=False, separator=',')\n#print(f'https://www.inaturalist.org/observations/identify?id={obs_ids}')\n\n# get just total results (count)\n#obs_count = await gettotalresults(endpoint_get_obs, req_params, use_authorization=False)\n#obs_count\n\n# get a series of counts\n#obs_count_series = [\n#    {'label': 'Texas 2020', 'year': 2020, 'place_id': 18},\n#    {'label': 'not Texas 2020', 'year': 2020, 'not_in_place': 18},\n#    {'label': 'Texas 2021', 'year': 2021, 'place_id': 18},\n#    {'label': 'not Texas 2021', 'year': 2021, 'not_in_place': 18},\n#]\n#await getcountseries(endpoint_get_obs, obs_count_series, ['year','place_id','not_in_place'], base_params=req_params, count_label='obs_count', use_authorization=False, add_count_to_series=True)\n#obs_count_series",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# if you order by id when you get observations (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\n# to use the code below, set get_more_obs = True before running.\nget_more_obs = False\n#if get_more_obs and obs and len(obs) >= endpoint_get_obs['max_records'] and len(obs) % endpoint_get_obs['max_records'] == 0:\nif get_more_obs and obs:\n    rp = deepcopy(req_params)\n    if rp.get('order_by','id') == 'id': # this only works if the records were sorted by id\n        if rp.get('order') == 'asc':\n            max_id = max([o.get('id') for o in obs])\n            print(f'getting additional observations for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above the obs we already have\n        else:\n            min_id = min([o.get('id') for o in obs])\n            print(f'getting additional observations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below the obs we already have\n        obs += await getobs(rp, get_all_pages=True, use_authorization=False)\n        print(f'observations accumulated: {len(obs)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Write Data to CSV",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport csv # used to output CSV files",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function write data to a CSV file\ndef datatocsv(data, csv_filename='export.csv'):\n    csv_fields = list(data[0].keys()) # get fields from the keys of the first record in the dataset\n    #print(len(data))\n    with open(csv_filename, 'w', newline='') as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n        csv_writer.writeheader()\n        csv_rows = 0\n        for r in data:\n            csv_writer.writerow(r)\n            csv_rows+=1\n        print (f'created CSV file {csv_filename} with {csv_rows} records.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# export to CSV\ndatatocsv(obs,'observations.csv')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Work with Data in a DataFrame",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# load data into a DataFrame (df)\ndf = pd.DataFrame(obs)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Get basic summary statistics for df\ndf.describe()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Preview the contents of the df\ndf",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# convert datetime columns to datetimes, localized to UTC\nfor k in ['time_observed_at','created_at','updated_at']:\n    if k in df.columns:\n        try:\n            df[k] = pd.to_datetime(df[k], utc=True, errors='coerce')\n        except:\n            print(f'could not convert column {k} datetime')\n\n# get count (of id) by observed year\ndf.groupby(df.time_observed_at.dt.year).id.count()\n\n# get count (of id) by created year\n#df.groupby(df.created_at.dt.year).id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# records where observation_fields are not null\ndf.loc[df.observation_fields.notnull()]\n\n# count (of id of) records where acc > 100\n# df.loc[df.public_positional_accuracy > 100].id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}