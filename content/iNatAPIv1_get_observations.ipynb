{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# iNaturalist API v1 Get Observations Example\n- Link: https://jumear.github.io/stirpy/lab?path=iNatAPIv1_get_observations.ipynb\n- GitHub Repo: https://github.com/jumear/stirpy",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Get Data from the iNaturalist API",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This example gets either a single or mutliple pages of results from the [API](https://api.inaturalist.org/). The requests are made asynchronously (in parallel, with a small incremental delay between the initiation of each page request), allowing large recordsets to be fetched in the shortest amount of time while respectng iNaturalist's [suggested request limit](https://www.inaturalist.org/pages/developers) (about 1 per second).\n\nThe example also provides a model for parsing the results, including flattening some of the items returned in the results (for use cases where the data is expected to be tabular, such as when exporting to a CSV file). For example, a single observation can be associated with multiple identifications, and this example code can flatten those multiple identifications into a single string so that all the identifications can be written out on a single line with the same observation row. This example also provides a model for client-side filtering (as an alternative when server-side filtering is not possible). The example shows how to get additional information, such as annotation descriptions and standard place information for observations.\n\nThere are also examples of how to get counts of observations or a series of counts (ex. observation counts by state).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nfrom urllib.parse import parse_qs # used for parsing URL parameters\nimport asyncio # used for asynchronous fetching\nimport math # used for a ceiling method\nfrom functools import partial # used for pre-loading functions with some arguments\nfrom datetime import datetime # used to convert string datetimes into actual datetimes\n\n# use Pyodide's pyfetch module if possible, but fall back to urllib3 outside of Pyodide\ntry:\n    from pyodide.http import pyfetch # Pyodide's fetch function (asynchronous)\n    use_pyfetch=True\nexcept:\n    #!pip install urllib3\n    import urllib3 # fall back to urllib3 if pyfetch isn't available. it can be made asynchronous using asynchio.to_thread().\n    use_pyfetch=False",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define custom functions used for getting data\n\n# function to turn a parameter string into a dict\ndef params_to_dict(params_string):\n    params_dict = parse_qs(params_string)\n    for p, v in params_dict.items():\n        if v: # iNaturalist handles multiple values for the same parameter using comma separated values. since parse_qs doesn't handle that situation, this section will handle it.\n            v = [(vv.split(',') if vv else vv) for vv in v]\n            params_dict[p] = [vvv for vv in v for vvv in vv]\n    return params_dict\n\n# function to combine a base url with a set of parameters. (there's a urlencode method in urllib.parse, but it's easier to get exactly what I need using this custom code.)\n# iNaturalist parameters are sometimes passed through the endpoint URL path rather than through the query string. so this handles that specific case.\ndef url_with_params(url_base, params=None):\n    if params is None:\n        params = {}\n    url = url_base\n    for p, v in params.items():\n        pv = ','.join(v)\n        if url.find(pp:=f'{{{p}}}') >= 0:\n            url = url.replace(pp, pv)\n        else:\n            s = '?' if url.find('?') < 0 else '&'\n            url += f'{s}{p}={pv}'\n    return url\n\n# basic function to fetch from API and convert repsonse to JSON\nasync def fetch_data(url, method='GET', use_authorization=False, delay=0):\n    await asyncio.sleep(delay)\n    req_headers = {}\n    if use_authorization and jwt:\n        req_headers = req_headers_base.copy() # make a copy\n        req_headers['Authorization'] = jwt\n    if use_pyfetch:\n        response = await pyfetch(url, method=method, headers=req_headers)\n        data = await response.json()\n    else:\n        response = await asyncio.to_thread(urllib3.request, method, url, headers=req_headers)\n        data = response.json()\n    print(f'Fetch complete: {method} {url}')\n    return data\n\n# function to GET total_results (count) from the API\nasync def get_total_results(endpoint, params=None, use_authorization=False, delay=0):\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    rp.pop('per_page', None) # remove per_page parameter, if it exists\n    rp['per_page'] = ['0'] # set this to 0, since we need only the count, not the actual records\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    total_results = int(data['total_results'])\n    print(f'Total records: {str(total_results)}')\n    return total_results\n\n# function to GET a single page of results from the API\n# additional parsing and additional filtering before and after the parsing can happen here, too\n# can be called directly but generally is intended to be called by get_results\nasync def get_results_single_page(endpoint, params=None, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None, delay=0):\n    if params is None:\n        params = {}\n    rp = params.copy() # make a copy\n    data = await fetch_data(url_with_params(endpoint['url'], rp), use_authorization=use_authorization, delay=delay)\n    results = data.get('results',[])\n    if pre_parse_filter_function:\n        results = list(filter(pre_parse_filter_function, results))\n    if parse_function:\n        results = parse_function(results)\n    if post_parse_filter_function:\n        results = list(filter(post_parse_filter_function, results))\n    return results\n\n# function to GET results from the API\n# if get_all_pages=True, get results over multiple pages using one of 2 methods:\n# 1. when endpoint URL path includes a key field, group ids into sets of up to a max number of records per page \n# 2. in other cases, get pages with the max records per page, up to the maximum limit that the API endpoint provides (generally 10000 records)\n# query pages in parallel, with each page having a incrementally delayed start. (iNaturalist suggests limiting requests to ~1 req/second.)\nasync def get_results(endpoint, params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    if params is None:\n        params = {}\n    results = []\n    if (page_key := endpoint.get('page_key')):\n        if not (page_key_values := params.get(page_key)):\n            print(f'Cannot query from this endpoint without values for {page_key} parameter')\n            return None\n        # if more values are input than the max per page, split these into multiple sets\n        max_per_page = endpoint['max_per_page']\n        total_key_values = len(page_key_values)\n        page_sets = [page_key_values[i:i+max_per_page] for i in range(0, total_key_values, max_per_page)]\n        print(f'There are {total_key_values} {page_key} values, requiring {len(page_sets)} API requests to retrieve. Retrieving {\"all sets\" if get_all_pages else \"only the first set\"}...')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in (range(len(page_sets) if get_all_pages else 1)):\n                rp = params.copy() # make a copy\n                rp[page_key] = page_sets[i]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    else:\n        max_page = math.ceil(endpoint['max_records'] / endpoint['max_per_page']) if get_all_pages else 1\n        if get_all_pages:\n            # when getting all pages, make a small query first to find how many total records there are.\n            # this allows us to calculate how many requests we need to make in total.\n            # if total records exceeds the maximum that the API will return, then retrieve only up to the maximum.\n            total_results = await get_total_results(endpoint, params, use_authorization)\n            total_pages = math.ceil(total_results / endpoint['max_per_page'])\n            if total_pages < max_page:\n                max_page = total_pages\n            print(f'Pages to retrieve: {str(max_page)}')\n        async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n            tasks = []\n            for i in range(max_page):\n                rp = params.copy() # make a copy\n                if get_all_pages:\n                    # if getting all pages, remove per_page and page parameters if they exist in the base params\n                    # and then set per_page = max and increment page for each request\n                    rp.pop('per_page', None)\n                    rp.pop('page', None)\n                    rp['per_page'] = [str(endpoint['max_per_page'])] # set this to the max if we're getting all pages\n                    rp['page'] = [str(i+1)]\n                tasks.append(tg.create_task(get_results_single_page(endpoint, params=rp, use_authorization=use_authorization, parse_function=parse_function, pre_parse_filter_function=pre_parse_filter_function, post_parse_filter_function=post_parse_filter_function, delay=i)))\n        for t in tasks:\n            results += t.result()\n    print(f'Total records retrieved: {str(len(results))}')\n    return results\n\n# function used by another function get_field_value to get a particular value from a results row\ndef get_ref_value(rec, ref):\n    # if the reference is chained (ex. taxon.id), then split these apart, and iterate through each object / dict.\n    # if any of the references is an index for an array / list (ex. index 0 in photos[0].id), then handle those, too.\n    value = rec\n    dict_chain = ref.split('.')\n    for r in dict_chain:\n        list_chain = [];\n        if r.find('[') >= 0:\n            r = r.replace(']','')\n            r = r.split('[')\n            list_chain = r[1:len(r)]\n            r = r[0]\n        value = value.get(r)\n        if list_chain and value is not None:\n            for i in map(int, list_chain):\n                if len(value) == 0:\n                    value = None\n                    break\n                value = value[i]\n        if value is None:\n            break\n    return value\n\n# function used by another function get_field_value to filter a nested list value based on certain filter parameters\ndef filter_ref_value(rec, value, params):\n    # filtered = [r for r in value if all([(get_ref_value(r,f.get('ref')) == (get_ref_value(rec, fvr) if (fvr := f.get('value_ref')) else f.get('value'))) for f in params])]\n    # the code below seems to run a tiny bit faster than the commented out line above\n    filtered = []\n    for r in value:\n        for f in params:\n            if not (get_ref_value(r,f.get('ref')) == (get_ref_value(rec, fvr) if (fvr := f.get('value_ref')) else f.get('value'))):\n                break\n        else:\n            filtered.append(r)\n    return filtered\n\n# function used by another function parse_results to parse a results row and get / calculate the value for a particular field\ndef get_field_value(rec, field):\n    # core processing\n    value = get_ref_value(rec, field['ref']) if field['ref'] else rec \n    if value is None and field.get('alt'):\n        value = get_ref_value(rec, field['alt'])\n    if (ff := field.get('function')) == 'count':\n        value = len(value) if value else 0\n    elif value is not None:\n        fp = field.get('params',{})\n        if ff == 'split':\n            value = value.split(fp.get('separator'))[fp.get('index')]\n        elif ff == 'join':\n            value = value = fp.get('separator').join(map(str, value)) if value else None\n        elif ff == 'replace':\n            value = value.replace(fp.get('old_text'), fp.get('new_text'))\n        elif ff == 'combine':\n            cvalue = fp.get('template','')\n            cref = fp.get('combine_refs',[])\n            for i, cr in enumerate(cref):\n                cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(value,cr)))\n            value = cvalue\n        elif ff == 'filter_combine':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = []\n            for r in filtered:\n                cvalue = fp.get('template','')\n                cref = fp.get('combine_refs',[])\n                for i, cr in enumerate(cref):\n                    cvalue = cvalue.replace(f'{{{i}}}',str(get_ref_value(r,cr)))\n                fvalue.append(cvalue)\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n            if value == []:\n                value = None\n        elif ff == 'filter_count':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            value = len(filtered) if (dr := fp.get('distinct_ref')) is None else len(set([get_ref_value(r, dr) for r in filtered])) # get a distinct count if distinct_ref is defined\n        elif ff == 'filter_select':\n            filtered = filter_ref_value(rec, value, fp.get('filter',[]))\n            fvalue = [get_ref_value(r, fp.get('select_ref')) for r in filtered]\n            value = fp.get('separator').join(map(str, fvalue)) if fvalue else None\n    if (cf := field.get('custom_function')):\n        value = cf(value)\n    if (cast_as := field.get('type')):\n        try:\n            value = cast_as(value)\n        except:\n            pass\n    return value\n\n# function used to parse a set of observation results, based on a set of parse_field defintiions\ndef parse_results(results, parse_fields, pre_parse_functions=None):\n    if pre_parse_functions is None:\n        pre_parse_functions = []\n    # parse based on the parse_fields defintion\n    presults = []\n    for r in results:\n        # special processing prior to core processing\n        for ppf in pre_parse_functions:\n            ppf(r)\n        # core processing\n        row = {}\n        for i, f in enumerate(parse_fields):\n            row[f.get('label') or f.get('ref') or f'col_{i+1}'] = get_field_value(r,f)\n        presults.append(row)\n    return presults\n\n# function to return only specific fields from the API results. (it basically drops unneeded items from the original results.)\n# only top-level items may be specified in the field list, but children of selected items will be included with the returned values.\ndef parse_simple(results, field_list):\n    return [{k: r.get(k) for k in field_list} for r in results]\n\n# function to get annotation ids and descriptions from the API\n# only the ids are included in the GET /v1/observations response. so a cross-reference is needed to translate the ids to plain English.\n# reults are stored in an attribute on the function called xref so that it won't be necessary to get data from the APi more than once\nasync def get_annotations():\n    xref = getattr(get_annotations, 'xref', None)\n    if xref is None:\n        xref = {};\n        terms = await(fetch_data(endpoint_get_controlled_terms['url']))\n        for t in terms['results']:\n            xref[t['id']] = t['label']\n            for v in t['values']:\n               xref[v['id']] = v['label']\n        print(f'Retrieved annnotation cross-references ({len(xref)} items)')\n        get_annotations.xref = xref\n    return xref\n\n# function intended to be used as pre-parse function in parse_results\n# add annotation descriptions from get_annotations to a set of observation results\ndef add_annotation_descriptions(r):\n    for a in r.get('annotations',[]):\n        a['controlled_attribute'] = get_annotations.xref[a['controlled_attribute_id']]\n        a['controlled_value'] = get_annotations.xref[a['controlled_value_id']]\n\n# function intended to be used as pre-parse function in parse_results\n# add a field with either taxon name + id (when obs field value is a taxon) or a plain value (when obs field value is any other kind of value)\ndef add_obs_field_taxon_or_value(r):\n    for of in r.get('ofvs',[]):\n        of['taxon_or_value'] = f'{of[\"taxon\"][\"name\"]} ({of[\"taxon\"][\"id\"]})' if of['datatype'] == 'taxon' and of.get('taxon') else of['value']\n\n# function intended to be used as pre-parse function in parse_results\n# add a filed that compares an identification's taxon vs the observation taxon (values = same, ancestor, descendant, different, or none)\ndef add_ident_vs_obs_comparison(r):\n    #ic = 0\n    for i, id in enumerate(r.get('identifications',[])):\n        #id['seq'] = i+1\n        #if id['current'] == 'true':\n        #    ic += 1\n        #    id['seq_current'] = ic\n        if not (rt := r.get('taxon')) or not (idt := id.get('taxon')):\n            id['vs_obs'] = 'none'\n        elif rt['id'] == idt['id']:\n            id['vs_obs'] = 'same'\n        elif (idta := idt.get('ancestry')) is not None and rt['id'] in map(int, idta.split('/')):\n            id['vs_obs'] = 'descendant'\n        elif (rta := rt.get('ancestry')) is not None and idt['id'] in map(int, rta.split('/')):\n            id['vs_obs'] = 'ancestor'\n        else:\n            id['vs_obs'] = 'different'\n\n# function intended to be used as pre-parse function in parse_results\n# the observation taxon itself has an ancestor list but no detailed ancestor information, but the taxon fields in the identiifcations do have ancestor details\n# so this will add ancestor details to the observation taxon, based on the ancestor details in the identifications (since the observation taxon should always be included in the indentification taxa or their ancestors)\ndef add_obs_taxon_ancestors(r):\n    ancestors = []\n    rank_level_kingdom = 70 # this is the highest-level taxon stored in identification[i].ancestors\n    if (rt := r.get('taxon')) and (taxon_id := rt.get('id')) is not None and (rank_level := rt.get('rank_level')) < rank_level_kingdom:\n        for id in r.get('identifications',[]):\n            if (idt := id.get('taxon')):\n                if idt['id'] == taxon_id:\n                    ancestors = list(idt['ancestors'])\n                    break\n                if (idta := idt['ancestors']):\n                    for i, atid in enumerate([a['id'] for a in idta]):\n                        if atid == taxon_id:\n                            ancestors = idta[0:i] # add everything above this taxon (will add this taxon later below)\n                            break\n                if ancestors:\n                    break\n    if rt and rank_level <= rank_level_kingdom:\n        ancestors.append(rt.copy())\n        rt['ancestors'] = ancestors\n\n# function to get and parse observations\nasync def get_obs(params=None, get_all_pages=False, use_authorization=False, parse_function=None, pre_parse_filter_function=None, post_parse_filter_function=None):\n    if params is None:\n        params = {}\n    pre_parse_functions = []\n    post_get_functions = []\n    if parse_function is None:\n        if params.get('only_id',['false']) == ['true']: # if only_id=true, then don't parse fields because only id will exist in the results\n            parse_function = None\n        else: # if a custom function is not specified, use parse_results with some default field definitions\n            # each dict in the field definition must have at least a ref (reference) key. (note: if ref is set to None, the observation row will be retrieved as the value.)\n            # use an optional label if you want the key to be different from the ref.\n            # use an optional type to cast the field to a specific data type.\n            # use an optional alt (alternative reference) if you want a fallback ref in case no data is found in ref.\n            # use optional function + params to do more complicated parsing of the ref,\n            # even more complicated logic can be handled with a custom_function, pre_parse_functions, or post_get_functions. \n            parse_fields = [\n                {'ref': 'id'},\n                #{'label': 'url', 'ref': None, 'function': 'combine', 'params': {'combine_refs': ['id'], 'template': 'https://www.inaturalist.org/observations/{0}'}},\n                #{'ref': 'uuid'},\n                {'ref': 'quality_grade'},\n                #{'label': 'user_id', 'ref': 'user.id'},\n                {'label': 'user_login', 'ref': 'user.login'},\n                #{'label': 'user_login_id', 'ref': 'user', 'function': 'combine', 'params': {'combine_refs': ['login','id'], 'template': '{0} ({1})'}},\n                #{'label': 'user_name', 'ref': 'user.name'},\n                #{'label': 'taxon_ancestors', 'ref': 'taxon.ancestors', 'function': 'filter_combine', 'params': {'combine_refs': ['name','rank','id'], 'template': '{0} ({1}) ({2})', 'separator': ', '}},\n                #{'label': 'kingdom', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'kingdom'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'phylum', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'phylum'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'class', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'class'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'order', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'order'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'family', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'family'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'genus', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'genus'}], 'select_ref': 'name', 'separator': ', '}},\n                #{'label': 'species', 'ref': 'taxon.ancestors', 'function': 'filter_select', 'params': {'filter': [{'ref': 'rank', 'value': 'species'}], 'select_ref': 'name', 'separator': ', '}},\n                {'label': 'taxon_id', 'ref': 'taxon.id'},\n                {'label': 'taxon_name', 'ref': 'taxon.name'},\n                {'label': 'taxon_preferred_common_name', 'ref': 'taxon.preferred_common_name'},\n                {'label': 'taxon_rank', 'ref': 'taxon.rank'},\n                #{'label': 'taxon_rank_level', 'ref': 'taxon.rank_level'},\n                #{'label': 'taxon_ancestry', 'ref': 'taxon.ancestry'},\n                #{'ref': 'observed_on_string'},\n                {'ref': 'time_observed_at'},\n                {'ref': 'created_at'},\n                #{'ref': 'updated_at'},\n                {'ref': 'place_guess'},\n                #{'ref': 'location'},\n                {'label': 'latitude', 'ref': 'location', 'type': float, 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n                {'label': 'longitude', 'ref': 'location', 'type': float, 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n                {'ref': 'public_positional_accuracy'},\n                #{'ref': 'private_place_guess'},\n                #{'ref': 'private_location'},\n                #{'label': 'private_latitude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 0}},\n                #{'label': 'private_longitiude', 'ref': 'private_location', 'function': 'split', 'params': {'separator': ',', 'index': 1}},\n                #{'ref': 'positional_accuracy'},\n                {'ref': 'taxon_geoprivacy'},\n                {'ref': 'privacy'},\n                {'ref': 'description'},\n                {'label': 'photos_count', 'ref':'photos', 'function': 'count'},\n                #{'label': 'photo_1_id', 'ref': 'photos[0].id'},\n                {'label': 'photo_1_url', 'ref': 'photos[0].url', 'function': 'replace', 'params': {'old_text': 'square', 'new_text': 'medium'}}, # size options are thumb, square, small, medium, large, and original\n                {'label': 'photo_1_license_code', 'ref': 'photos[0].license_code'},\n                {'label': 'sounds_count', 'ref':'sounds', 'function': 'count'},\n                {'ref': 'comments_count'},\n                #{'label': 'others_current_identifications_count', 'ref': 'identifications_count'},\n                {'label': 'current_identifications_count', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'current', 'value': True}]}},\n                #{'label': 'current_identifications_by_observer', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}]}},\n                #{'label': 'current_identification_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}], 'select_ref': 'taxon.name', 'separator': ', '}},\n                #{'label': 'current_identification_category_by_observer', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'user.id', 'value_ref': 'user.id'}, {'ref': 'current', 'value': True}], 'select_ref': 'category', 'separator': ', '}},\n                #{'ref': 'owners_identification_from_vision'},\n                {'label': 'prefers_community_taxon', 'ref': 'preferences.prefers_community_taxon', 'alt': 'user.preferences.prefers_community_taxa'},\n                #{'label': 'identifier_ids', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.id', 'separator': ', '}},\n                #{'label': 'identifier_logins', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'user.login', 'separator': ', '}},\n                {'label': 'identifications', 'ref': 'identifications', 'function': 'filter_combine', 'params': {'filter': [{'ref': 'current', 'value': True}], 'combine_refs': ['user.login','taxon.name','taxon.id'], 'template': '{0}: {1} ({2})', 'separator': ', '}},\n                #{'label': 'days_to_first_id', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(first_id_date) - datetime.fromisoformat(x['created_at'])).days if (first_id_date := get_ref_value(x,'identifications[0].created_at')) is not None else None)},\n                {'label': 'days_to_first_id_by_observer', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(first_id_by_observer_date[0]) - datetime.fromisoformat(x['created_at'])).days if (first_id_by_observer_date := [xi['created_at'] for xi in x.get('identifications') if (xi['user']['id']==x['user']['id'])]) else None)},\n                {'label': 'days_to_first_id_by_others', 'ref': None, 'custom_function': (lambda x: (datetime.fromisoformat(first_id_by_others_date[0]) - datetime.fromisoformat(x['created_at'])).days if (first_id_by_others_date := [xi['created_at'] for xi in x.get('identifications') if (xi['user']['id']!=x['user']['id'])]) else None)},\n                #{'label': 'identification_date_first', 'ref': 'identifications[0].created_at'},\n                #{'label': 'identification_date_last', 'ref': 'identifications[-1].created_at'},\n                #{'label': 'identifications_vs_obs', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'current', 'value': True}], 'select_ref': 'vs_obs', 'separator': ', '}},\n                {'label': 'identifications_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'same'}, {'ref': 'current', 'value': True}]}},\n                #{'label': 'ident_taxa_vs_obs_same', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'same'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'ancestor'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_ancestor', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'ancestor'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'descendant'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_descendant', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'descendant'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                #{'label': 'identifications_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'different'}, {'ref': 'current', 'value': True}]}},\n                {'label': 'ident_taxa_vs_obs_different', 'ref': 'identifications', 'function': 'filter_count', 'params': {'filter': [{'ref': 'vs_obs', 'value': 'different'}, {'ref': 'current', 'value': True}], 'distinct_ref': 'taxon.id'}},\n                {'label': 'ident_disagreement_vs_obs', 'ref': 'identifications', 'function': 'filter_select', 'params': {'filter': [{'ref': 'disagreement', 'value': True}, {'ref': 'current', 'value': True}], 'select_ref': 'vs_obs', 'separator': ', '}},\n                {'label': 'reviewed_by_count', 'ref': 'reviewed_by', 'function': 'count'},\n                #{'ref': 'reviewed_by', 'function': 'join', 'params': {'separator':', '}},\n                #{'ref': 'captive'},\n                {'label': 'annotations_count','ref':'annotations', 'function': 'count'},\n                #{'label': 'annotations_ids', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute_id','controlled_value_id'], 'template': '{0}:{1}', 'separator': ', '}},\n                {'label': 'annotations', 'ref': 'annotations', 'function': 'filter_combine', 'params': {'combine_refs': ['controlled_attribute','controlled_value'], 'template': '{0}: {1}', 'separator': ', '}}, # note: this relies on some pre-procesing to create the controlled_attribute and controlled_value fields\n                {'label': 'observation_fields_count', 'ref':'ofvs', 'function': 'count'},\n                #{'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','value'], 'template': '{0} ({1}): {2}', 'separator': '; '}},\n                {'label': 'observation_fields', 'ref': 'ofvs', 'function': 'filter_combine', 'params': {'combine_refs': ['name','field_id','taxon_or_value'], 'template': '{0} ({1}): {2}', 'separator': '; '}}, # note: this relies on pre-procesing to create a field that contains either taxon or value\n                {'label': 'tags_count', 'ref':'tags', 'function': 'count'},\n                {'ref': 'tags', 'function': 'join', 'params': {'separator': ', '}},\n                #{'ref': 'oauth_application_id'},\n                #{'ref': 'site_id'},\n                {'label': 'gbif_occurence_url', 'ref': 'outlinks', 'function': 'filter_select', 'params': {'filter': [{'ref': 'source', 'value': 'GBIF'}], 'select_ref': 'url', 'separator': ', '}},\n                #{'ref': 'place_ids'}, # if this is retrieved, this will be replaced with standard place information (later via a post-get function)\n            ]\n            # use pre-parse functions and post-get functions to handle more complicated stiuations\n            parse_field_refs = [pf.get('ref') for pf in parse_fields]\n            if 'annotations' in parse_field_refs:\n                await get_annotations()\n                pre_parse_functions.append(add_annotation_descriptions)\n            if 'ofvs' in parse_field_refs:\n                pre_parse_functions.append(add_obs_field_taxon_or_value)\n            if 'identifications' in parse_field_refs:\n                pre_parse_functions.append(add_ident_vs_obs_comparison)\n            if 'taxon.ancestors' in parse_field_refs:\n                pre_parse_functions.append(add_obs_taxon_ancestors)\n            if 'place_ids' in parse_field_refs:\n                post_get_functions.append(add_std_places)\n            parse_function = partial(parse_results, parse_fields=parse_fields, pre_parse_functions=pre_parse_functions) # pre-load parse_fields with these parse_fields and pre_parse_functions\n    print('Getting observations...')\n    results = await get_results(endpoint_get_obs, params, get_all_pages, use_authorization, parse_function, pre_parse_filter_function, post_parse_filter_function)\n    for pgf in post_get_functions:\n        try:\n            await pgf(results) # assume async function by default\n        except:\n            pgf(results) # fall back to regular execution for non-async functions\n    return results\n\n# function to add human-friendly standard place information to obs, if places_ids are included in (parsed) results\n# this is intended to be run after getting all pages of observations (to minimize the number of requests to get place data)\nasync def add_std_places(obs, remove_place_ids=True):\n    # get a unique list of place_ids associated with the observations\n    place_ids = [pid if (pid := o.get('place_ids')) else [] for o in obs]\n    place_ids = set([pp for p in place_ids for pp in p])\n    if not place_ids:\n        return obs\n    print('Adding standard place info to observations...')\n    # request info from the API for only the \"standard\" places (continents, plus country-, state-, county-, and town-equivalent places)\n    admin_level_xref = { # define these in the order these should be displayed in the results\n        #'30': 'town', # these are available only for certain states in the USA\n        '20':'county',\n        '10':'state',\n        '0':'country',\n        '-10': 'continent',\n    }\n    place_req_params = {'id': list(map(str,place_ids)), 'admin_level': list(admin_level_xref.keys())}\n    std_places = await get_results(endpoint_get_places, place_req_params, get_all_pages=True, parse_function=partial(parse_simple, field_list=['id','admin_level','name','slug']))\n    if std_places is None:\n        std_places = []\n    # add std place info to observations\n    for o in obs:\n        osp = [sp for sp in std_places if sp['id'] in op] if (op := o.get('place_ids')) else [] # find standard places associated with this obs\n        for alc, ald in admin_level_xref.items():\n            alp = [ospp for ospp in osp if ospp['admin_level'] == int(alc)] # find the place with this admin level\n            o[f'std_place_{ald}'] = alp[0]['name'] if alp else None\n        if remove_place_ids: \n            o.pop('place_ids',None) # remove this column from the parsed results, since it's no longer needed\n    return obs\n\n# function to get a series of counts\n# base_params are the (fixed) parameters that will be applied when getting the count for each item in the series.\n# series_params is a list of (variable) parameters (keys) to add to base_params for each item in the series.\n# series is a list of dicts, each of which defines the parameter key/value pairs for each item in the series.\n# each item in the series list can contain additional attributes that are not parameters, and it does not have to contain all the keys in the series_params list.\n# if add_count_to_series is set to True, the function will add the counts to the original series object; otherwise, it just returns a (deep) copy of series with counts.\nasync def get_count_series(endpoint, series, series_params, base_params=None, count_label='rec_count', use_authorization=False, add_count_to_series=False):\n    if base_params is None:\n        base_params = {}\n    if not series or not series_params:\n        print(f'The series parameter must be a list of dicts with keys that include the values in the list passed in for series_params.')\n        return None\n    rv = []\n    results = series if add_count_to_series else series.copy() # return values will look the same, but if add_count_to_series=True, the original series list wlll actually change\n    async with asyncio.TaskGroup() as tg: # available in Python 3.11+\n        tasks = []\n        for i, r in enumerate(results):\n            rp = base_params.copy()\n            for sp in series_params:\n                if (spv := r.get(sp)) is not None:\n                    rp.pop(sp, None)\n                    rp[sp] = [str(spv)]\n            tasks.append(tg.create_task(get_total_results(endpoint, rp, use_authorization=use_authorization, delay=i)))\n    for i, t in enumerate(tasks):\n        if not add_count_to_series:\n            results[i] = results[i].copy() # return values will look the same, but if add_count_to_series=True, the original series list wlll actually change\n        results[i][count_label] = t.result()\n    return results\n\n# function to string together a list of observation ids into sets of up to a max number of observations per set\n# the original intended use case is to create URLs linking to the iNaturalist Explore or Identification page, filtered for specific observations\ndef obs_ids_to_sets(obs_ids, max_set_size=500, separator=',', prefix=''):\n    obs_id_sets = []\n    for i in range(0, len(obs_ids), max_set_size):\n        obs_id_string = prefix + separator.join(map(str, obs_ids[i:i+max_set_size]))\n        obs_id_sets.append(obs_id_string)\n        print(f'Set {int(i/max_set_size+1)}: {obs_id_string}')\n    return obs_id_sets",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# define the parameters needed for your request\nreq_params_string = 'verifiable=true&spam=false'\nreq_params = params_to_dict(req_params_string)\nreq_headers_base = {'Content-Type': 'application/json', 'Accept': 'application/json'}\n\n# to make authorized calls, set jwt to the \"api_token\" value from https://www.inaturalist.org/users/api_token.\n# the JWT is valid for 24 hours. it can be used to do / access anything your iNat account can access. so keep it safe, and don't share it.\n# you will also have to set use_authorization=True when making your API request below.\njwt = None\n\n# define endpoints\nendpoint_get_obs = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/observations',\n    'max_records': 10000,\n    'max_per_page': 200,\n}\nendpoint_get_controlled_terms = {\n    'method': 'GET' ,\n    'url': 'https://api.inaturalist.org/v1/controlled_terms',\n}\nendpoint_get_places = {\n    'method': 'GET',\n    'url': 'https://api.inaturalist.org/v1/places/{id}',\n    'max_per_page': 500,\n    'page_key': 'id'\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# main execution section\n\n# get observations\nobs = await get_obs(req_params, get_all_pages=False, use_authorization=False)\n#obs\n\n# when possible, it's always best to filter on the server side by using filter parameters when making API requests.\n# but when a particular filter is not available in the API, it may still be possible to filter on the client side (as opposed to server side)\n# here's an example of how to do client-side filtering for observations which have >1 (current) identification using post_parse_filter_function\n# use pre_parse_filter_function when you can filter based on the results directly from the API response.\n# use post_parse_filter_function when you must rely on the values in a parsed field to do the filtering.\n# (you can always filter separately *after* getting observations, of course, but filtering *while* getting obs saves on system memory when getting multiple pages of results from the API.)\n#obs = await get_obs(req_params, get_all_pages=False, use_authorization=False, post_parse_filter_function=(lambda x: x['current_identifications_count'] > 1))\n\n# get observation ids from obs\n#obs_ids = [o.get('id') for o in obs]\n#obs_id_sets = obs_ids_to_sets(obs_ids, prefix='https://www.inaturalist.org/observations/identify?id=')\n\n# get just total results (count)\n#obs_count = await get_total_results(endpoint_get_obs, req_params, use_authorization=False)\n#obs_count\n\n# get a series of counts\n#obs_count_series = [\n#    {'label': 'Texas 2020', 'year': 2020, 'place_id': 18},\n#    {'label': 'not Texas 2020', 'year': 2020, 'not_in_place': 18},\n#    {'label': 'Texas 2021', 'year': 2021, 'place_id': 18},\n#    {'label': 'not Texas 2021', 'year': 2021, 'not_in_place': 18},\n#]\n#await get_count_series(endpoint_get_obs, obs_count_series, ['year','place_id','not_in_place'], base_params=req_params, count_label='obs_count', use_authorization=False, add_count_to_series=True)\n#obs_count_series",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# if you order by id when you get observations (this is the default behavior if you don't specify an order_by parameter), \n# then it should be possible to work around the max 10000 record limit of the API by using the id_above or id_below parameters.\n# i purposely am not automating this process completely (because I don't want to make it too easy to accidentally get a ton of data),\n# but i'm including this bit of code here to provide an idea of how to do it.\n# to use the code below, set get_more_obs = True before running.\nget_more_obs = False\n#if get_more_obs and obs and len(obs) >= endpoint_get_obs['max_records'] and len(obs) % endpoint_get_obs['max_records'] == 0:\nif get_more_obs and obs:\n    rp = req_params.copy() # make a copy\n    if rp.get('order_by',['id']) == ['id']: # this only works if the records were sorted by id\n        if rp.get('order',['desc']) == ['asc']:\n            max_id = max([o.get('id') for o in obs])\n            print(f'Getting additional observations for id_above={max_id}')\n            rp.pop('id_above', None) # remove per_page parameter, if it exists\n            rp['id_above'] = [str(max_id)] # set this to the max_id so that the records we get will have ids above those of the obs we already have\n        else:\n            min_id = min([o.get('id') for o in obs])\n            print(f'Getting additional observations for id_below={min_id}')\n            rp.pop('id_below', None) # remove per_page parameter, if it exists\n            rp['id_below'] = [str(min_id)] # set this to the min_id so that the records we get will have ids below those of the obs we already have\n        obs += await get_obs(rp, get_all_pages=True, use_authorization=False)\n        print(f'Observations accumulated: {len(obs)}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Write Data to CSV",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Ths takes the results retrieved above and writes them to a CSV file. The file will appear in the main folder of the file tree (the topmost tab in the left pane of the JupyterLab interface). Files generated in JupyterLite are saved to the browser's storage. So those will need to be downloaded to a more permanent location if they need to be archived more permanently.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport csv # used to output CSV files",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function write data to a CSV file\ndef data_to_csv(data, csv_filename='export.csv'):\n    csv_fields = list(data[0]) # get fields from the keys of the first record in the dataset\n    with open(csv_filename, 'w', newline='') as csv_file:\n        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fields)\n        csv_writer.writeheader()\n        csv_writer.writerows(data)\n        print(f'Created CSV file {csv_filename} with {len(data)} records.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# export to CSV\ndata_to_csv(obs,'observations.csv')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Work with Data in a DataFrame",
      "metadata": {
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Since many Python analysis / visualization modules and workflows rely on getting data into a `pandas` dataframe, this provides a very barebones example of getting the data into a dataframe. The dataframe should generally handle most of the data type conversions, but there's a little bit more effort to get dates into a datetime typed column in the dataframe.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load required modules\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# load data into a DataFrame (df)\ndf = pd.DataFrame(obs)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Preview the contents of the df\ndf",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Get basic summary statistics for df\ndf.describe()",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# convert datetime columns to datetimes, localized to UTC\nfor k in ['time_observed_at','created_at','updated_at']:\n    if k in df.columns:\n        try:\n            df[k] = pd.to_datetime(df[k], utc=True, errors='coerce')\n        except:\n            print(f'Could not convert column {k} to datetime')\n\n# get count (of id) by observed year\ndf.groupby(df.time_observed_at.dt.year).id.count()\n\n# get count (of id) by created year\n#df.groupby(df.created_at.dt.year).id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# filtering example: records where observation_fields are not null\ndf.loc[df.observation_fields.notnull()]\n\n# count (of id of) records where acc > 100\n# df.loc[df.public_positional_accuracy > 100].id.count()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}